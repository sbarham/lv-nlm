{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ours\n",
    "from corpus.ptb import PTB\n",
    "import util\n",
    "from util.utils import to_var, expierment_name\n",
    "from models.bowman import SentenceVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    splits = [util.TRAIN, util.VAL] + ([util.TEST] if args.test else [])\n",
    "    datasets = OrderedDict()\n",
    "    datasets.splits = splits\n",
    "    \n",
    "    # create train, validation, and possibly test split\n",
    "    for split in datasets.splits:\n",
    "        datasets[split] = PTB(\n",
    "            data_dir=args.data_dir,\n",
    "            split=split,\n",
    "            create_data=args.create_data,\n",
    "            max_sequence_length=args.max_sequence_length,\n",
    "            min_occ=args.min_occ,\n",
    "            embeddings=args.embeddings\n",
    "        )\n",
    "    \n",
    "    # return the splits\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(args, datasets):\n",
    "    model = SentenceVAE(\n",
    "            vocab_size=datasets[util.TRAIN].vocab_size,\n",
    "            sos_idx=datasets[util.TRAIN].sos_idx,\n",
    "            eos_idx=datasets[util.TRAIN].eos_idx,\n",
    "            pad_idx=datasets[util.TRAIN].pad_idx,\n",
    "            unk_idx=datasets[util.TRAIN].unk_idx,\n",
    "            max_sequence_length=args.max_sequence_length,\n",
    "            embedding_size=args.embedding_size,\n",
    "            rnn_type=args.rnn_type,\n",
    "            hidden_size=args.hidden_size,\n",
    "            word_dropout=args.word_dropout,\n",
    "            embedding_dropout=args.embedding_dropout,\n",
    "            latent_size=args.latent_size,\n",
    "            num_layers=args.num_layers,\n",
    "            bidirectional=args.bidirectional\n",
    "        )\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_anneal_function(anneal_function, step, k, x0):\n",
    "    if anneal_function == 'logistic':\n",
    "        return float(1.0 / (1.0 + np.exp(-k * (step - x0))))\n",
    "    elif anneal_function == 'linear':\n",
    "        return min(1.0, step / x0)\n",
    "    elif anneal_function == 'const':\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(logp, target, length, mean, logv, anneal_function, step, k, x0, pad_idx):\n",
    "    NLL = torch.nn.NLLLoss(reduction='sum', ignore_index=pad_idx)\n",
    "    \n",
    "    # cut-off unnecessary padding from target, and flatten\n",
    "    target = target[:, :torch.max(length).data[0]].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(2))\n",
    "        \n",
    "    # Negative Log Likelihood\n",
    "    NLL_loss = NLL(logp, target)\n",
    "\n",
    "    # KL Divergence\n",
    "    KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n",
    "    KL_weight = kl_anneal_function(anneal_function, step, k, x0)\n",
    "\n",
    "    return NLL_loss, KL_loss, KL_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx2word(sents, i2w, pad_idx):\n",
    "    sent_str = [str()] * len(sents)\n",
    "\n",
    "    for sent_idx, sent in enumerate(sents):\n",
    "        for word_id in sent:\n",
    "            try:\n",
    "                word_id = word_id.item()\n",
    "            except: pass\n",
    "            \n",
    "            if word_id == pad_idx:\n",
    "                break\n",
    "            \n",
    "            sent_str[sent_idx] += (i2w[word_id] + \" \")\n",
    "\n",
    "        sent_str[sent_idx] = sent_str[sent_idx].strip()\n",
    "\n",
    "\n",
    "    return sent_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model/Runtime Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Dataset/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, datasets, args):\n",
    "    print(model)\n",
    "    \n",
    "    timestamp = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
    "\n",
    "    # create the directory for saving this model\n",
    "    args.save_model_path = os.path.join(util.MODEL_DIR, timestamp)\n",
    "    os.makedirs(args.save_model_path)\n",
    "    \n",
    "    # create the optimizer, the tracker, and initialize the step to 0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    trackers = {split: defaultdict(list) for split in datasets.splits}\n",
    "    step = 0\n",
    "    best = float('inf')\n",
    "    \n",
    "    # get the pad index, for convenience\n",
    "    pad_idx = datasets['train'].get_w2i()['<pad>']\n",
    "    \n",
    "    # go!\n",
    "    for epoch in range(args.epochs):\n",
    "        for split in datasets.splits:\n",
    "            print(\"SPLIT = {}\".format(split))\n",
    "            \n",
    "            data_loader = DataLoader(\n",
    "                dataset=datasets[split],\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=split=='train',\n",
    "                num_workers=cpu_count(),\n",
    "                pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            # Enable/Disable Dropout\n",
    "            if split == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for iteration, batch in enumerate(data_loader):\n",
    "                batch_size = batch['input'].size(0)\n",
    "\n",
    "                for k, v in batch.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        batch[k] = to_var(v)\n",
    "\n",
    "                # Forward pass\n",
    "                logp, mean, logv, z = model(batch['input'], batch['length'])\n",
    "\n",
    "                # loss calculation\n",
    "                NLL_loss, KL_loss, KL_weight = loss_fn(logp, batch['target'],\n",
    "                    batch['length'], mean, logv, args.anneal_function, step, args.k, args.x0, pad_idx)\n",
    "\n",
    "                loss = (NLL_loss + KL_weight * KL_loss) / batch_size\n",
    "\n",
    "                # backward + optimization\n",
    "                if split == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    step += 1\n",
    "\n",
    "                if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                    print(\"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"\n",
    "                        % (split.upper(),\n",
    "                           iteration,\n",
    "                           len(data_loader) - 1,\n",
    "                           loss.data[0],\n",
    "                           NLL_loss.data[0] / batch_size,\n",
    "                           KL_loss.data[0] / batch_size,\n",
    "                           KL_weight))\n",
    "\n",
    "                trackers[split]['ELBO'].append(loss.item())\n",
    "                trackers[split]['NLL'].append(NLL_loss.data[0] / batch_size)\n",
    "                trackers[split]['KLL'].append(KL_loss.data[0] / batch_size)\n",
    "                trackers[split]['KL_weight'].append(KL_weight)\n",
    "                \n",
    "#                 if split == 'valid':\n",
    "#                     i2w = datasets['train'].get_i2w()\n",
    "#                     trackers[split]['target_sents'] += idx2word(batch['target'].data, i2w=i2w, pad_idx=pad_idx)\n",
    "#                     trackers[split]['z'].append(z.tolist())\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            END OF BATCH\n",
    "            \"\"\"\n",
    "            print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\" % (split.upper(), epoch, args.epochs, np.mean(trackers[split]['ELBO'])))\n",
    "\n",
    "            # save a dump of all sentences and the encoded latent space\n",
    "#             if split == 'valid':\n",
    "#                 dump = {'target_sents':trackers[split]['target_sents'], 'z':trackers[split]['z']}\n",
    "#                 if not os.path.exists(os.path.join('dumps', ts)):\n",
    "#                     os.makedirs('dumps/' + ts)\n",
    "#                 with open(os.path.join('dumps/'+ts+'/valid_E%i.pickle' % epoch), 'wb') as dump_file:\n",
    "#                     pickle.dump(dump, dump_file)\n",
    "\n",
    "            # save checkpoint\n",
    "            if split == 'train':                \n",
    "                # save checkpoint\n",
    "                checkpoint_path = os.path.join(args.save_model_path, \"E%i.pytorch\" % (epoch))\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(\"Model saved at %s\" % checkpoint_path)\n",
    "                \n",
    "                # check if best checkpoint so far\n",
    "                if np.mean(trackers[split]['ELBO']) < best:\n",
    "                    best = np.mean(trackers[split]['ELBO'])\n",
    "                    args.load_checkpoint = 'E{}.pytorch'.format(epoch)\n",
    "                \n",
    "    return trackers, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exponential_smoothing(ys, beta=0.8, ub=math.inf, lb=-math.inf):\n",
    "    \"\"\"\n",
    "    This is ugly, and I should have used a comprehension, but\n",
    "    it'll get the job done. I made it a function because I suspect\n",
    "    I may need it later.\n",
    "    \"\"\"\n",
    "    smooth_ys = [ys[0]]\n",
    "    for y in ys:\n",
    "        if y > ub or y < lb:\n",
    "            smooth_ys.append(smooth_ys[-1])\n",
    "        else:\n",
    "            smooth_ys.append(beta * smooth_ys[-1] + (1 - beta) * y)\n",
    "    return smooth_ys[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(ELBO, NLL, KL, title, fname=None, xlabel=\"Epochs\", ylabel=\"Measurements\", hline=None, epochs=None):\n",
    "    \"\"\"\n",
    "    Just a *slight* abstraction over pyplot to ease development a bit.\n",
    "    \"\"\"\n",
    "    xs = list(range(len(ELBO)))\n",
    "    if epochs is not None:\n",
    "        xs = [x / len(xs) * epochs for x in xs]\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    if hline:\n",
    "        plt.axhline(y=hline, color='r', linestyle='-')\n",
    "    \n",
    "    plt.plot(xs, ELBO, label=\"ELBO\")\n",
    "    plt.plot(xs, NLL, label=\"NLL Loss\", c='blue')\n",
    "    plt.plot(xs, KL, label=\"KL Loss\", c='red')\n",
    "    plt.legend()\n",
    "    \n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_elbo(ELBO, fname=None, title='ELBO', xlabel=\"Epochs\", ylabel=\"ELBO\", hline=None, epochs=None):\n",
    "    \"\"\"\n",
    "    Just a *slight* abstraction over pyplot to ease development a bit.\n",
    "    \"\"\"\n",
    "    xs = list(range(len(ELBO)))\n",
    "    if epochs is not None:\n",
    "        xs = [x / len(xs) * epochs for x in xs]\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    plt.plot(xs, ELBO, label=\"ELBO\")\n",
    "    plt.legend()\n",
    "        \n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(trackers, datasets, args):\n",
    "    for split in datasets.splits:\n",
    "        fname = '{}_perf:emb{}-z{}-lstm{}-maxlen{}'.format(\n",
    "            split,\n",
    "            args.embedding_size,\n",
    "            args.latent_size,\n",
    "            args.hidden_size,\n",
    "            args.max_sequence_length\n",
    "        )\n",
    "        \n",
    "        fname = os.path.join(args.save_model_path, fname)\n",
    "        \n",
    "        plot(\n",
    "            fname=fname,\n",
    "            ELBO=exponential_smoothing(trackers[split]['ELBO']),\n",
    "            KL=exponential_smoothing(trackers[split]['KLL']),\n",
    "            NLL=exponential_smoothing(trackers[split]['NLL']),\n",
    "            title='S-VAE *{}* Performance\\n(Mikolov\\'s Simplified PTB, max length={})'.format(\n",
    "                split,\n",
    "                args.max_sequence_length\n",
    "            ),\n",
    "            epochs=args.epochs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolate(start, end, steps):\n",
    "    steps = steps + 2\n",
    "    \n",
    "    interpolation = np.zeros((start.shape[0], steps))\n",
    "\n",
    "    for dim, (s, e) in enumerate(zip(start, end)):\n",
    "        interpolation[dim] = np.linspace(s, e, steps)\n",
    "\n",
    "    return interpolation.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_args(args):\n",
    "    fname = os.path.join(args.save_model_path, 'args')\n",
    "    with open(fname, 'w+') as file:\n",
    "        lines = ['{}: {}\\n'.format(key, val) for key, val in vars(args).items()]\n",
    "        file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_trackers(trackers, args):\n",
    "    fname = os.path.join(args.save_model_path, 'trackers.pickle')\n",
    "    with open(fname, 'wb') as file:\n",
    "        pickle.dump(trackers, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(args):\n",
    "    with open(args.data_dir + '/ptb.vocab.pickle', 'rb') as file:\n",
    "        vocab = pickle.load(file)\n",
    "\n",
    "    w2i, i2w = vocab['w2i'], vocab['i2w']\n",
    "\n",
    "    model = SentenceVAE(\n",
    "        vocab_size=len(w2i),\n",
    "        sos_idx=w2i['<sos>'],\n",
    "        eos_idx=w2i['<eos>'],\n",
    "        pad_idx=w2i['<pad>'],\n",
    "        unk_idx=w2i['<unk>'],\n",
    "        max_sequence_length=args.max_sequence_length,\n",
    "        embedding_size=args.embedding_size,\n",
    "        rnn_type=args.rnn_type,\n",
    "        hidden_size=args.hidden_size,\n",
    "        word_dropout=args.word_dropout,\n",
    "        embedding_dropout=args.embedding_dropout,\n",
    "        latent_size=args.latent_size,\n",
    "        num_layers=args.num_layers,\n",
    "        bidirectional=args.bidirectional\n",
    "        )\n",
    "\n",
    "    checkpoint_path = os.path.join(args.save_model_path, args.load_checkpoint)\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(checkpoint_path)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "    print(\"Model loaded from %s\" % (checkpoint_path))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    fname = os.path.join(args.save_model_path, 'samples')\n",
    "    lines = []\n",
    "    with open(fname, 'w+') as file:\n",
    "        samples, z = model.inference(n=args.num_samples)\n",
    "        lines += ['----------SAMPLES----------']\n",
    "        lines += [line + '\\n' for line in idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])]\n",
    "        lines += ['\\n']\n",
    "\n",
    "        z1 = torch.randn([args.latent_size]).numpy()\n",
    "        z2 = torch.randn([args.latent_size]).numpy()\n",
    "        z = to_var(torch.from_numpy(interpolate(start=z1, end=z2, steps=8)).float())\n",
    "        samples, _ = model.inference(z=z)\n",
    "        lines += ['-------SELF-GENERATED INTERPOLATION-------']\n",
    "        lines += [line + '\\n' for line in idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])]\n",
    "        lines += ['\\n']\n",
    "\n",
    "        # pick two random sentences\n",
    "        i = random.randint(0, len(datasets['train']))\n",
    "        j = random.randint(0, len(datasets['train']))\n",
    "\n",
    "        s_i = torch.tensor([datasets['train'][i]['input']])\n",
    "        s_j = torch.tensor([datasets['train'][j]['input']])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, _, _, z_i = model(s_i, torch.tensor([1]))\n",
    "            _, _, _, z_j = model(s_j, torch.tensor([1]))\n",
    "            \n",
    "        z1, z2 = z_i.squeeze().numpy(), z_j.squeeze().numpy()\n",
    "        z = to_var(torch.from_numpy(interpolate(start=z1, end=z2, steps=8)).float())\n",
    "        samples, _ = model.inference(z=z)\n",
    "        lines += ['-------DATA-DRIVEN INTERPOLATION----------']\n",
    "        lines += [line + '\\n'  for line in idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])]\n",
    "        lines += ['\\n']\n",
    "        \n",
    "        print(\"wrote samples to '{}'\".format(fname))\n",
    "        file.writelines(lines)\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sentences(datasets):\n",
    "    data = datasets['train'] + datasets['valid'] + datasets['test']\n",
    "    \n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(datasets):\n",
    "    data = datasets['train'] + datasets['valid'] + datasets['test']\n",
    "    total = 0\n",
    "    for sent in data:\n",
    "        total = total + sent['length']\n",
    "        \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set all model/runtime arguments\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.data_dir = 'data'\n",
    "args.create_data = True\n",
    "args.max_sequence_length = 20\n",
    "args.min_occ = 1\n",
    "args.test = True\n",
    "args.epochs = 10\n",
    "args.batch_size = 64\n",
    "args.learning_rate = 0.001\n",
    "\n",
    "args.num_samples = 10\n",
    "\n",
    "args.embeddings = True\n",
    "args.embedding_size = 300\n",
    "args.rnn_type = 'gru'\n",
    "args.hidden_size = 256\n",
    "args.num_layers = 1\n",
    "args.bidirectional = False\n",
    "args.latent_size = 16\n",
    "args.word_dropout = 0.0\n",
    "args.embedding_dropout = 0.5\n",
    "\n",
    "args.anneal_function = 'logistic'\n",
    "args.k = 0.0025\n",
    "args.x0 = 2500\n",
    "\n",
    "args.print_every = 50\n",
    "args.tensorboard_logging = False\n",
    "args.logdir = 'logs'\n",
    "args.save_model_path = 'bin/good25'\n",
    "args.load_checkpoint = 'E9.pytorch'\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear', 'const']\n",
    "assert 0 <= args.word_dropout <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(args):\n",
    "    # create the datasets and model\n",
    "    datasets = create_datasets(args)\n",
    "\n",
    "    # create a new model\n",
    "    model = create_model(args, datasets)\n",
    "    \n",
    "    # train the model and record its performance\n",
    "    trackers, model = train(model, datasets, args)\n",
    "    \n",
    "    # write args to file\n",
    "    save_args(args)\n",
    "    \n",
    "    # save the trackers\n",
    "    save_trackers(trackers, args)\n",
    "    \n",
    "    # graph the results and save\n",
    "    graph(trackers, datasets, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Penn Treebank *train* data:\n",
      "------------------------------------------\n",
      "Creating vocab file ...\n",
      "\t[Getting word counts]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c999773f09c4879a5813fb3c86e8b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t[Creating dictionaries]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa503570b7a4444dbb9af000352bb6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t[Loading pretrained GLOVE embeddings -- this may take a while the first time]\n",
      "Loaded 400000 words\n",
      "\t[Mapping vocab to GLOVE embeddings]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e5ec2d3f164195839a1d5ee6fd99ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary created (10009 word types)!\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31826ebfef1423d87ba9e7f75bb9a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset created, with:\n",
      "\t17144 sentences\n",
      "\t236015 word tokens\n",
      "\t13.76662389174055 avg. sentence length\n",
      "\n",
      "\n",
      "Preprocessing Penn Treebank *valid* data:\n",
      "------------------------------------------\n",
      "Loading vocab file ...\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202b0b0d22f8417998bb93ba4369a2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset created, with:\n",
      "\t1410 sentences\n",
      "\t19458 word tokens\n",
      "\t13.8 avg. sentence length\n",
      "\n",
      "\n",
      "Preprocessing Penn Treebank *test* data:\n",
      "------------------------------------------\n",
      "Loading vocab file ...\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0d655ff0c44f38b373faf9c2245063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset created, with:\n",
      "\t1511 sentences\n",
      "\t20319 word tokens\n",
      "\t13.447385837193911 avg. sentence length\n",
      "\n",
      "\n",
      "SentenceVAE(\n",
      "  (embedding): Embedding(10009, 300)\n",
      "  (embedding_dropout): Dropout(p=0.5)\n",
      "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
      "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
      "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
      "  (outputs2vocab): Linear(in_features=256, out_features=10009, bias=True)\n",
      ")\n",
      "SPLIT = train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \"\"\"\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/267, Loss  127.5258, NLL-Loss  127.5251, KL-Loss    0.3850, KL-Weight  0.002\n",
      "TRAIN Batch 0050/267, Loss   89.2702, NLL-Loss   89.2422, KL-Loss   12.8154, KL-Weight  0.002\n",
      "TRAIN Batch 0100/267, Loss   82.6268, NLL-Loss   82.5285, KL-Loss   39.7600, KL-Weight  0.002\n",
      "TRAIN Batch 0150/267, Loss   78.2589, NLL-Loss   78.1154, KL-Loss   51.2425, KL-Weight  0.003\n",
      "TRAIN Batch 0200/267, Loss   78.8180, NLL-Loss   78.6278, KL-Loss   59.9471, KL-Weight  0.003\n",
      "TRAIN Batch 0250/267, Loss   72.0963, NLL-Loss   71.8758, KL-Loss   61.3545, KL-Weight  0.004\n",
      "TRAIN Batch 0267/267, Loss   69.3677, NLL-Loss   69.1415, KL-Loss   60.3285, KL-Weight  0.004\n",
      "TRAIN Epoch 00/10, Mean ELBO   78.4488\n",
      "Model saved at bin/2019-Jan-17-03:39:21/E0.pytorch\n",
      "SPLIT = valid\n",
      "VALID Batch 0000/22, Loss   71.6888, NLL-Loss   71.4573, KL-Loss   61.5902, KL-Weight  0.004\n",
      "VALID Batch 0022/22, Loss   81.1978, NLL-Loss   81.0003, KL-Loss   52.5654, KL-Weight  0.004\n",
      "VALID Epoch 00/10, Mean ELBO   67.7878\n",
      "SPLIT = test\n",
      "TEST Batch 0000/23, Loss   64.7758, NLL-Loss   64.5385, KL-Loss   63.1579, KL-Weight  0.004\n",
      "TEST Batch 0023/23, Loss   61.9200, NLL-Loss   61.6762, KL-Loss   64.8610, KL-Weight  0.004\n",
      "TEST Epoch 00/10, Mean ELBO   65.7065\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/267, Loss   63.3996, NLL-Loss   63.1623, KL-Loss   63.1407, KL-Weight  0.004\n",
      "TRAIN Batch 0050/267, Loss   66.9502, NLL-Loss   66.6543, KL-Loss   69.5375, KL-Weight  0.004\n",
      "TRAIN Batch 0100/267, Loss   56.8622, NLL-Loss   56.5067, KL-Loss   73.7454, KL-Weight  0.005\n",
      "TRAIN Batch 0150/267, Loss   64.3577, NLL-Loss   63.9780, KL-Loss   69.5544, KL-Weight  0.005\n",
      "TRAIN Batch 0200/267, Loss   68.2908, NLL-Loss   67.8391, KL-Loss   73.0602, KL-Weight  0.006\n",
      "TRAIN Batch 0250/267, Loss   59.4530, NLL-Loss   58.9386, KL-Loss   73.4985, KL-Weight  0.007\n",
      "TRAIN Batch 0267/267, Loss   58.9341, NLL-Loss   58.3761, KL-Loss   76.4387, KL-Weight  0.007\n",
      "TRAIN Epoch 01/10, Mean ELBO   70.5178\n",
      "Model saved at bin/2019-Jan-17-03:39:21/E1.pytorch\n",
      "SPLIT = valid\n",
      "VALID Batch 0000/22, Loss   66.9622, NLL-Loss   66.4529, KL-Loss   69.5971, KL-Weight  0.007\n",
      "VALID Batch 0022/22, Loss   72.0927, NLL-Loss   71.6511, KL-Loss   60.3420, KL-Weight  0.007\n",
      "VALID Epoch 01/10, Mean ELBO   64.6764\n",
      "SPLIT = test\n",
      "TEST Batch 0000/23, Loss   58.1927, NLL-Loss   57.6664, KL-Loss   71.9096, KL-Weight  0.007\n",
      "TEST Batch 0023/23, Loss   56.3584, NLL-Loss   55.8287, KL-Loss   72.3771, KL-Weight  0.007\n",
      "TEST Epoch 01/10, Mean ELBO   62.5699\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/267, Loss   55.3007, NLL-Loss   54.7395, KL-Loss   76.6777, KL-Weight  0.007\n",
      "TRAIN Batch 0050/267, Loss   57.3412, NLL-Loss   56.6599, KL-Loss   82.2313, KL-Weight  0.008\n",
      "TRAIN Batch 0100/267, Loss   63.2309, NLL-Loss   62.4657, KL-Loss   81.6002, KL-Weight  0.009\n",
      "TRAIN Batch 0150/267, Loss   57.5382, NLL-Loss   56.7357, KL-Loss   75.6096, KL-Weight  0.011\n",
      "TRAIN Batch 0200/267, Loss   53.6260, NLL-Loss   52.7504, KL-Loss   72.9122, KL-Weight  0.012\n",
      "TRAIN Batch 0250/267, Loss   53.5111, NLL-Loss   52.5365, KL-Loss   71.7289, KL-Weight  0.014\n",
      "TRAIN Batch 0267/267, Loss   50.5930, NLL-Loss   49.6283, KL-Loss   68.0912, KL-Weight  0.014\n",
      "TRAIN Epoch 02/10, Mean ELBO   65.5351\n",
      "Model saved at bin/2019-Jan-17-03:39:21/E2.pytorch\n",
      "SPLIT = valid\n",
      "VALID Batch 0000/22, Loss   64.8761, NLL-Loss   63.9162, KL-Loss   67.5899, KL-Weight  0.014\n",
      "VALID Batch 0022/22, Loss   66.9237, NLL-Loss   66.0541, KL-Loss   61.2259, KL-Weight  0.014\n",
      "VALID Epoch 02/10, Mean ELBO   62.8333\n",
      "SPLIT = test\n",
      "TEST Batch 0000/23, Loss   55.6366, NLL-Loss   54.6510, KL-Loss   69.3991, KL-Weight  0.014\n",
      "TEST Batch 0023/23, Loss   54.7978, NLL-Loss   53.8199, KL-Loss   68.8530, KL-Weight  0.014\n",
      "TEST Epoch 02/10, Mean ELBO   60.7212\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/267, Loss   58.0488, NLL-Loss   57.0232, KL-Loss   72.2064, KL-Weight  0.014\n",
      "TRAIN Batch 0050/267, Loss   51.9407, NLL-Loss   50.6974, KL-Loss   77.3935, KL-Weight  0.016\n",
      "TRAIN Batch 0100/267, Loss   51.0031, NLL-Loss   49.6328, KL-Loss   75.4415, KL-Weight  0.018\n",
      "TRAIN Batch 0150/267, Loss   51.9599, NLL-Loss   50.4944, KL-Loss   71.3725, KL-Weight  0.021\n",
      "TRAIN Batch 0200/267, Loss   53.5192, NLL-Loss   51.9930, KL-Loss   65.7721, KL-Weight  0.023\n"
     ]
    }
   ],
   "source": [
    "datasets = run_experiment(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate samples and interpolations\n",
    "# test(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
