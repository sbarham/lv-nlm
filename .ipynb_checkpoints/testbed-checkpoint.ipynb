{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ours\n",
    "from corpus.ptb import PTB\n",
    "from corpus.brown import Brown\n",
    "from corpus.gutenberg import Gutenberg\n",
    "from corpus.kjv import Bible\n",
    "from corpus.wikitext_2 import Wikitext2\n",
    "from corpus.wikitext_103 import Wikitext103\n",
    "import util\n",
    "from util.utils import to_var, expierment_name\n",
    "from models.bowman import SentenceVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    # select correct corpus class\n",
    "    assert args.corpus in ['ptb', 'bible', 'gutenberg', 'brown', 'wikitext-2', 'wikitext-103']\n",
    "    if args.corpus == 'ptb':\n",
    "        corpus_class = PTB\n",
    "    elif args.corpus == 'kjv' or args.corpus == 'bible':\n",
    "        nltk.download('gutenberg')\n",
    "        corpus_class = Bible\n",
    "    elif args.corpus == 'gutenberg':\n",
    "        nltk.download('gutenberg')\n",
    "        corpus_class = Gutenberg\n",
    "    elif args.corpus == 'brown':\n",
    "        nltk.download('brown')\n",
    "        corpus_class = Brown\n",
    "    elif args.corpus == 'wikitext-2':\n",
    "        corpus_class = Wikitext2\n",
    "    elif args.corpus == 'wikitext-103':\n",
    "        corpus_class = Wikitext103\n",
    "    \n",
    "    # prepare for splits\n",
    "    splits = [util.TRAIN, util.VAL] + ([util.TEST] if args.test else [])\n",
    "    datasets = OrderedDict()\n",
    "    datasets.splits = splits\n",
    "    \n",
    "    # create train, validation, and possibly test split\n",
    "    for split in datasets.splits:\n",
    "        datasets[split] = corpus_class(\n",
    "            data_dir=args.data_dir,\n",
    "            split=split,\n",
    "            create_data=args.create_data,\n",
    "            max_sequence_length=args.max_sequence_length,\n",
    "            min_occ=args.min_occ,\n",
    "            embeddings=args.embeddings\n",
    "        )\n",
    "    \n",
    "    # return the splits\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(args, datasets):\n",
    "    model = SentenceVAE(\n",
    "            vocab_size=datasets[util.TRAIN].vocab_size,\n",
    "            sos_idx=datasets[util.TRAIN].sos_idx,\n",
    "            eos_idx=datasets[util.TRAIN].eos_idx,\n",
    "            pad_idx=datasets[util.TRAIN].pad_idx,\n",
    "            unk_idx=datasets[util.TRAIN].unk_idx,\n",
    "            max_sequence_length=args.max_sequence_length,\n",
    "            embedding_size=args.embedding_size,\n",
    "            rnn_type=args.rnn_type,\n",
    "            hidden_size=args.hidden_size,\n",
    "            word_dropout=args.word_dropout,\n",
    "            embedding_dropout=args.embedding_dropout,\n",
    "            latent_size=args.latent_size,\n",
    "            num_layers=args.num_layers,\n",
    "            bidirectional=args.bidirectional\n",
    "        )\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_anneal_function(anneal_function, step, k, x0):\n",
    "    if anneal_function == 'logistic':\n",
    "        return float(1.0 / (1.0 + np.exp(-k * (step - x0))))\n",
    "    elif anneal_function == 'linear':\n",
    "        return min(1.0, step / x0)\n",
    "    elif anneal_function == 'const':\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(logp, target, length, mean, logv, anneal_function, step, k, x0, pad_idx):\n",
    "    NLL = torch.nn.NLLLoss(reduction='sum', ignore_index=pad_idx)\n",
    "    \n",
    "    # cut-off unnecessary padding from target, and flatten\n",
    "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(2))\n",
    "        \n",
    "    # Negative Log Likelihood\n",
    "    NLL_loss = NLL(logp, target)\n",
    "\n",
    "    # KL Divergence\n",
    "    KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n",
    "    KL_weight = kl_anneal_function(anneal_function, step, k, x0)\n",
    "\n",
    "    return NLL_loss, KL_loss, KL_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx2word(sents, i2w, pad_idx):\n",
    "    sent_str = [str()] * len(sents)\n",
    "\n",
    "    for sent_idx, sent in enumerate(sents):\n",
    "        for word_id in sent:\n",
    "            try:\n",
    "                word_id = word_id.item()\n",
    "            except: pass\n",
    "            \n",
    "            if word_id == pad_idx:\n",
    "                break\n",
    "            \n",
    "            sent_str[sent_idx] += (i2w[word_id] + \" \")\n",
    "\n",
    "        sent_str[sent_idx] = sent_str[sent_idx].strip()\n",
    "\n",
    "\n",
    "    return sent_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model/Runtime Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Dataset/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, datasets, args):\n",
    "    print(model)\n",
    "    \n",
    "    timestamp = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
    "\n",
    "    # create the directory for saving this model\n",
    "    args.save_model_path = os.path.join(util.MODEL_DIR, timestamp)\n",
    "    os.makedirs(args.save_model_path)\n",
    "    \n",
    "    # create the optimizer, the tracker, and initialize the step to 0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    trackers = {split: defaultdict(list) for split in datasets.splits}\n",
    "    step = 0\n",
    "    best = float('inf')\n",
    "    \n",
    "    # get the pad index, for convenience\n",
    "    pad_idx = datasets['train'].get_w2i()['<pad>']\n",
    "    \n",
    "    # go!\n",
    "    for epoch in range(args.epochs):\n",
    "        for split in datasets.splits:\n",
    "            print(\"SPLIT = {}\".format(split))\n",
    "            \n",
    "            data_loader = DataLoader(\n",
    "                dataset=datasets[split],\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=split=='train',\n",
    "                num_workers=cpu_count(),\n",
    "                pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            # Enable/Disable Dropout\n",
    "            if split == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for iteration, batch in enumerate(data_loader):\n",
    "                batch_size = batch['input'].size(0)\n",
    "\n",
    "                for k, v in batch.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        batch[k] = to_var(v)\n",
    "\n",
    "                # Forward pass\n",
    "                logp, mean, logv, z = model(batch['input'], batch['length'])\n",
    "\n",
    "                # loss calculation\n",
    "                NLL_loss, KL_loss, KL_weight = loss_fn(logp, batch['target'],\n",
    "                    batch['length'], mean, logv, args.anneal_function, step, args.k, args.x0, pad_idx)\n",
    "\n",
    "                loss = (NLL_loss + KL_weight * KL_loss) / batch_size\n",
    "\n",
    "                # backward + optimization\n",
    "                if split == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    step += 1\n",
    "\n",
    "                if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                    print(\"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"\n",
    "                        % (split.upper(),\n",
    "                           iteration,\n",
    "                           len(data_loader) - 1,\n",
    "                           loss.item(),\n",
    "                           NLL_loss.item() / batch_size,\n",
    "                           KL_loss.item() / batch_size,\n",
    "                           KL_weight))\n",
    "\n",
    "                trackers[split]['ELBO'].append(loss.item())\n",
    "                trackers[split]['NLL'].append(NLL_loss.item() / batch_size)\n",
    "                trackers[split]['KLL'].append(KL_loss.item() / batch_size)\n",
    "                trackers[split]['KL_weight'].append(KL_weight)\n",
    "                \n",
    "#                 if split == 'valid':\n",
    "#                     i2w = datasets['train'].get_i2w()\n",
    "#                     trackers[split]['target_sents'] += idx2word(batch['target'].data, i2w=i2w, pad_idx=pad_idx)\n",
    "#                     trackers[split]['z'].append(z.tolist())\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            END OF BATCH\n",
    "            \"\"\"\n",
    "            print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\" % (split.upper(), epoch, args.epochs, np.mean(trackers[split]['ELBO'])))\n",
    "\n",
    "            # save a dump of all sentences and the encoded latent space\n",
    "#             if split == 'valid':\n",
    "#                 dump = {'target_sents':trackers[split]['target_sents'], 'z':trackers[split]['z']}\n",
    "#                 if not os.path.exists(os.path.join('dumps', ts)):\n",
    "#                     os.makedirs('dumps/' + ts)\n",
    "#                 with open(os.path.join('dumps/'+ts+'/valid_E%i.pickle' % epoch), 'wb') as dump_file:\n",
    "#                     pickle.dump(dump, dump_file)\n",
    "\n",
    "            # save checkpoint\n",
    "            if split == 'train':                \n",
    "                # save checkpoint\n",
    "                checkpoint_path = os.path.join(args.save_model_path, \"E%i.pytorch\" % (epoch))\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(\"Model saved at %s\" % checkpoint_path)\n",
    "                \n",
    "                # check if best checkpoint so far\n",
    "                if np.mean(trackers[split]['ELBO']) < best:\n",
    "                    best = np.mean(trackers[split]['ELBO'])\n",
    "                    args.load_checkpoint = 'E{}.pytorch'.format(epoch)\n",
    "                \n",
    "    return trackers, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exponential_smoothing(ys, beta=0.8, ub=math.inf, lb=-math.inf):\n",
    "    \"\"\"\n",
    "    This is ugly, and I should have used a comprehension, but\n",
    "    it'll get the job done. I made it a function because I suspect\n",
    "    I may need it later.\n",
    "    \"\"\"\n",
    "    smooth_ys = [ys[0]]\n",
    "    for y in ys:\n",
    "        if y > ub or y < lb:\n",
    "            smooth_ys.append(smooth_ys[-1])\n",
    "        else:\n",
    "            smooth_ys.append(beta * smooth_ys[-1] + (1 - beta) * y)\n",
    "    return smooth_ys[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(ELBO, NLL, KL, title, fname=None, xlabel=\"Epochs\", ylabel=\"Measurements\", hline=None, epochs=None):\n",
    "    \"\"\"\n",
    "    Just a *slight* abstraction over pyplot to ease development a bit.\n",
    "    \"\"\"\n",
    "    xs = list(range(len(ELBO)))\n",
    "    if epochs is not None:\n",
    "        xs = [x / len(xs) * epochs for x in xs]\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    if hline:\n",
    "        plt.axhline(y=hline, color='r', linestyle='-')\n",
    "    \n",
    "    plt.plot(xs, ELBO, label=\"ELBO\")\n",
    "    plt.plot(xs, NLL, label=\"NLL Loss\", c='blue')\n",
    "    plt.plot(xs, KL, label=\"KL Loss\", c='red')\n",
    "    plt.legend()\n",
    "    \n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_elbo(ELBO, fname=None, title='ELBO', xlabel=\"Epochs\", ylabel=\"ELBO\", hline=None, epochs=None):\n",
    "    \"\"\"\n",
    "    Just a *slight* abstraction over pyplot to ease development a bit.\n",
    "    \"\"\"\n",
    "    xs = list(range(len(ELBO)))\n",
    "    if epochs is not None:\n",
    "        xs = [x / len(xs) * epochs for x in xs]\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    plt.plot(xs, ELBO, label=\"ELBO\")\n",
    "    plt.legend()\n",
    "        \n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(trackers, datasets, args):\n",
    "    for split in datasets.splits:\n",
    "        fname = '{}_perf:emb{}-z{}-lstm{}-maxlen{}'.format(\n",
    "            split,\n",
    "            args.embedding_size,\n",
    "            args.latent_size,\n",
    "            args.hidden_size,\n",
    "            args.max_sequence_length\n",
    "        )\n",
    "        \n",
    "        fname = os.path.join(args.save_model_path, fname)\n",
    "        \n",
    "        plot(\n",
    "            fname=fname,\n",
    "            ELBO=exponential_smoothing(trackers[split]['ELBO']),\n",
    "            KL=exponential_smoothing(trackers[split]['KLL']),\n",
    "            NLL=exponential_smoothing(trackers[split]['NLL']),\n",
    "            title='S-VAE *{}* Performance\\n(Mikolov\\'s Simplified PTB, max length={})'.format(\n",
    "                split,\n",
    "                args.max_sequence_length\n",
    "            ),\n",
    "            epochs=args.epochs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolate(start, end, steps):\n",
    "    steps = steps + 2\n",
    "    \n",
    "    interpolation = np.zeros((start.shape[0], steps))\n",
    "\n",
    "    for dim, (s, e) in enumerate(zip(start, end)):\n",
    "        interpolation[dim] = np.linspace(s, e, steps)\n",
    "\n",
    "    return interpolation.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_args(args):\n",
    "    fname = os.path.join(args.save_model_path, 'args')\n",
    "    with open(fname, 'w+') as file:\n",
    "        lines = ['{}: {}\\n'.format(key, val) for key, val in vars(args).items()]\n",
    "        file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_trackers(trackers, args):\n",
    "    fname = os.path.join(args.save_model_path, 'trackers.pickle')\n",
    "    with open(fname, 'wb') as file:\n",
    "        pickle.dump(trackers, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(args):\n",
    "    with open(args.data_dir + '/ptb.vocab.pickle', 'rb') as file:\n",
    "        vocab = pickle.load(file)\n",
    "\n",
    "    w2i, i2w = vocab['w2i'], vocab['i2w']\n",
    "\n",
    "    model = SentenceVAE(\n",
    "        vocab_size=len(w2i),\n",
    "        sos_idx=w2i['<sos>'],\n",
    "        eos_idx=w2i['<eos>'],\n",
    "        pad_idx=w2i['<pad>'],\n",
    "        unk_idx=w2i['<unk>'],\n",
    "        max_sequence_length=args.max_sequence_length,\n",
    "        embedding_size=args.embedding_size,\n",
    "        rnn_type=args.rnn_type,\n",
    "        hidden_size=args.hidden_size,\n",
    "        word_dropout=args.word_dropout,\n",
    "        embedding_dropout=args.embedding_dropout,\n",
    "        latent_size=args.latent_size,\n",
    "        num_layers=args.num_layers,\n",
    "        bidirectional=args.bidirectional\n",
    "        )\n",
    "\n",
    "    checkpoint_path = os.path.join(args.save_model_path, args.load_checkpoint)\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(checkpoint_path)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "    print(\"Model loaded from %s\" % (checkpoint_path))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    fname = os.path.join(args.save_model_path, 'samples')\n",
    "    lines = []\n",
    "    with open(fname, 'w+') as file:\n",
    "        samples, z = model.inference(n=args.num_samples)\n",
    "        lines += ['----------SAMPLES----------']\n",
    "        lines += [line + '\\n' for line in idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])]\n",
    "        lines += ['\\n']\n",
    "\n",
    "        z1 = torch.randn([args.latent_size]).numpy()\n",
    "        z2 = torch.randn([args.latent_size]).numpy()\n",
    "        z = to_var(torch.from_numpy(interpolate(start=z1, end=z2, steps=8)).float())\n",
    "        samples, _ = model.inference(z=z)\n",
    "        lines += ['-------SELF-GENERATED INTERPOLATION-------']\n",
    "        lines += [line + '\\n' for line in idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])]\n",
    "        lines += ['\\n']\n",
    "\n",
    "        # pick two random sentences\n",
    "        i = random.randint(0, len(datasets['train']))\n",
    "        j = random.randint(0, len(datasets['train']))\n",
    "\n",
    "        s_i = torch.tensor([datasets['train'][i]['input']])\n",
    "        s_j = torch.tensor([datasets['train'][j]['input']])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, _, _, z_i = model(s_i, torch.tensor([1]))\n",
    "            _, _, _, z_j = model(s_j, torch.tensor([1]))\n",
    "            \n",
    "        z1, z2 = z_i.squeeze().numpy(), z_j.squeeze().numpy()\n",
    "        z = to_var(torch.from_numpy(interpolate(start=z1, end=z2, steps=8)).float())\n",
    "        samples, _ = model.inference(z=z)\n",
    "        lines += ['-------DATA-DRIVEN INTERPOLATION----------']\n",
    "        lines += [line + '\\n'  for line in idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])]\n",
    "        lines += ['\\n']\n",
    "        \n",
    "        print(\"wrote samples to '{}'\".format(fname))\n",
    "        file.writelines(lines)\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sentences(datasets):\n",
    "    data = datasets['train'] + datasets['valid'] + datasets['test']\n",
    "    \n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(datasets):\n",
    "    data = datasets['train'] + datasets['valid'] + datasets['test']\n",
    "    total = 0\n",
    "    for sent in data:\n",
    "        total = total + sent['length']\n",
    "        \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set all model/runtime arguments\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.data_dir = 'data'\n",
    "args.create_data = True\n",
    "args.max_sequence_length = 50\n",
    "args.min_occ = 1\n",
    "args.test = True\n",
    "args.epochs = 10\n",
    "args.batch_size = 64\n",
    "args.learning_rate = 0.001\n",
    "\n",
    "args.corpus = 'bible'\n",
    "\n",
    "args.num_samples = 10\n",
    "\n",
    "args.embeddings = True\n",
    "args.embedding_size = 300\n",
    "args.rnn_type = 'gru'\n",
    "args.hidden_size = 512\n",
    "args.num_layers = 1\n",
    "args.bidirectional = False\n",
    "args.latent_size = 32\n",
    "args.word_dropout = 0.0\n",
    "args.embedding_dropout = 0.5\n",
    "\n",
    "args.anneal_function = 'logistic'\n",
    "args.k = 0.0025\n",
    "args.x0 = 2500\n",
    "\n",
    "args.print_every = 50\n",
    "args.tensorboard_logging = False\n",
    "args.logdir = 'logs'\n",
    "args.save_model_path = 'bin/good25'\n",
    "args.load_checkpoint = 'E9.pytorch'\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear', 'const']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "assert args.corpus in ['ptb', 'bible', 'gutenberg', 'brown', 'wikitext-2', 'wikitext-103']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(args):\n",
    "    # create the datasets and model\n",
    "    datasets = create_datasets(args)\n",
    "\n",
    "    # create a new model\n",
    "    model = create_model(args, datasets)\n",
    "    \n",
    "    # train the model and record its performance\n",
    "    trackers, model = train(model, datasets, args)\n",
    "    \n",
    "    # write args to file\n",
    "    save_args(args)\n",
    "    \n",
    "    # save the trackers\n",
    "    save_trackers(trackers, args)\n",
    "    \n",
    "    # graph the results and save\n",
    "    graph(trackers, datasets, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'corpus_class' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-db6d6e041109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-159-f8592c6d711c>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# create the datasets and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# create a new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-1fb9c70ef090>\u001b[0m in \u001b[0;36mcreate_datasets\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# create train, validation, and possibly test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         datasets[split] = corpus_class(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'corpus_class' referenced before assignment"
     ]
    }
   ],
   "source": [
    "datasets = run_experiment(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate samples and interpolations\n",
    "# test(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
