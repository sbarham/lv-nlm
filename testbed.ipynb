{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "\n",
    "# ours\n",
    "from corpus.utils import create_datasets as create_datasets\n",
    "from corpus.utils import idx2word as idx2word\n",
    "\n",
    "from plotting.utils import exponential_smoothing as exponential_smoothing\n",
    "from plotting.utils import plot as plot\n",
    "from plotting.utils import plot_elbo as plot_elbo\n",
    "from plotting.utils import graph as graph\n",
    "\n",
    "from experiment.utils import args_to_dict as args_to_dict\n",
    "from experiment.utils import save_args as save_args\n",
    "from experiment.utils import save_model_printout as save_model_printout\n",
    "from experiment.utils import save_trackers as save_trackers\n",
    "from experiment.utils import pretty_print_trackers as pretty_print_trackers\n",
    "from experiment.utils import convert as convert\n",
    "from experiment.sample import test as test\n",
    "from experiment.sample import interpolate as interpolate\n",
    "from experiment.sample import random_samples as random_samples\n",
    "from experiment.sample import cold_interpolation as cold_interpolation\n",
    "from experiment.sample import warm_interpolation as warm_interpolation\n",
    "from experiment.sample import reconstruction as reconstruction\n",
    "\n",
    "import util\n",
    "from util.utils import to_var, expierment_name\n",
    "\n",
    "from models.utils import create_model as create_model\n",
    "from models.utils import kl_anneal_function as kl_anneal_function\n",
    "from models.utils import loss_fn as loss_fn\n",
    "from models.utils import train as train\n",
    "from models.bowman import SentenceVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model/Data Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set all model/runtime arguments\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.data_dir = 'data'\n",
    "args.create_data = True\n",
    "args.max_sequence_length = 50\n",
    "args.min_occ = 1\n",
    "args.test = True\n",
    "args.epochs = 10\n",
    "args.batch_size = 64\n",
    "args.learning_rate = 0.001\n",
    "\n",
    "args.corpus = 'bible'\n",
    "\n",
    "args.num_samples = 10\n",
    "args.num_steps = 10\n",
    "args.num_cold_interpolations = 5\n",
    "args.num_warm_interpolations = 5\n",
    "args.num_reconstructions = 5\n",
    "args.sample_warmup_period = 100\n",
    "\n",
    "args.embeddings = True\n",
    "args.embedding_size = 300\n",
    "args.rnn_type = 'gru'\n",
    "args.hidden_size = 256\n",
    "args.num_layers = 1\n",
    "args.bidirectional = True\n",
    "args.latent_size = 20\n",
    "args.word_dropout = 0.0\n",
    "args.embedding_dropout = 0.5\n",
    "\n",
    "args.anneal_function = 'logistic'\n",
    "args.k = 0.0025\n",
    "args.x0 = 2500\n",
    "\n",
    "args.print_every = 50\n",
    "args.tensorboard_logging = False\n",
    "args.logdir = 'logs'\n",
    "args.save_model_path = 'bin/test'\n",
    "# args.load_checkpoint = 'E9.pytorch'\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear', 'const']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "assert args.corpus in ['ptb', 'bible', 'gutenberg', 'brown', 'wikitext-2', 'wikitext-103']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help(args, verbose=False):\n",
    "    # create the datasets and model\n",
    "    datasets = create_datasets(args, verbose=verbose)\n",
    "\n",
    "    # create a new model\n",
    "    model = create_model(args, datasets)\n",
    "    \n",
    "    # train the model and record its performance\n",
    "    trackers, model = train(model, datasets, args, verbose=verbose)\n",
    "    \n",
    "    return datasets, model, trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(args, verbose=False):\n",
    "    # create the datasets and model\n",
    "    datasets = create_datasets(args, verbose=verbose)\n",
    "\n",
    "    # create a new model\n",
    "    model = create_model(args, datasets)\n",
    "    \n",
    "    # train the model and record its performance\n",
    "    trackers, model = train(model, datasets, args, verbose=verbose)\n",
    "    # args.best_epoch = 3\n",
    "    # args.load_checkpoint = 'E3.pytorch'\n",
    "    \n",
    "    # write args to file\n",
    "    save_args(args)\n",
    "    save_model_printout(args, model)\n",
    "    \n",
    "    # save the trackers\n",
    "    save_trackers(trackers, args)\n",
    "    \n",
    "    # graph the results and save\n",
    "    graph(trackers, datasets, args)\n",
    "    \n",
    "    # run the inference/sampling code on the trained model\n",
    "    # and save the results\n",
    "    test(args, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets, model, trackers = help(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'hidden_size': [256, 512, 1024],\n",
    "    'corpus': ['ptb', 'brown'],\n",
    "    'max_sequence_length': [20, 35, 50],\n",
    "    'latent_size': [32],\n",
    "    'bidirectional': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(parameters):\n",
    "    for parameter_set in itertools.product(*[parameters[k] for k in parameters]):\n",
    "        for i, key in enumerate(parameters.keys()):\n",
    "            # change the desired attributes of args\n",
    "            setattr(args, key, parameter_set[i])\n",
    "        # run the modified experiment\n",
    "        run_experiment(args, verbose='False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending model to cuda\n",
      "SentenceVAE(\n",
      "  (embedding): Embedding(10009, 300)\n",
      "  (embedding_dropout): Dropout(p=0.5)\n",
      "  (encoder_rnn): GRU(300, 256, batch_first=True, bidirectional=True)\n",
      "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (hidden2mean): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (hidden2logv): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (latent2hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (outputs2vocab): Linear(in_features=256, out_features=10009, bias=True)\n",
      ")\n",
      "Beginning training at: 2019-Feb-01-14:24:19\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      " EPOCH 1, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss  116.5624, NLL-Loss  116.5614, KL-Loss    0.4850, KL-Weight  0.002\n",
      "TRAIN Batch 0050/267, Loss   76.3122, NLL-Loss   76.2576, KL-Loss   25.0142, KL-Weight  0.002\n",
      "TRAIN Batch 0100/267, Loss   73.7497, NLL-Loss   73.6660, KL-Loss   33.8856, KL-Weight  0.002\n",
      "TRAIN Batch 0150/267, Loss   81.2749, NLL-Loss   81.1793, KL-Loss   34.1610, KL-Weight  0.003\n",
      "TRAIN Batch 0200/267, Loss   76.9210, NLL-Loss   76.8011, KL-Loss   37.8021, KL-Weight  0.003\n",
      "TRAIN Batch 0250/267, Loss   74.8973, NLL-Loss   74.7717, KL-Loss   34.9432, KL-Weight  0.004\n",
      "TRAIN Batch 0267/267, Loss   79.5049, NLL-Loss   79.3913, KL-Loss   30.2997, KL-Weight  0.004\n",
      "TRAIN Epoch 01/10, Mean ELBO   80.6407\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E1.pytorch\n",
      " EPOCH 1, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   77.1848, NLL-Loss   77.0658, KL-Loss   31.6513, KL-Weight  0.004\n",
      "VAL Batch 0022/22, Loss   84.4228, NLL-Loss   84.3124, KL-Loss   29.3850, KL-Weight  0.004\n",
      "VAL Epoch 01/10, Mean ELBO   74.0554\n",
      " EPOCH 1, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   70.3402, NLL-Loss   70.2219, KL-Loss   31.4768, KL-Weight  0.004\n",
      "TEST Batch 0023/23, Loss   68.0107, NLL-Loss   67.8938, KL-Loss   31.1229, KL-Weight  0.004\n",
      "TEST Epoch 01/10, Mean ELBO   71.8284\n",
      "\n",
      "\n",
      " EPOCH 2, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   73.7718, NLL-Loss   73.6502, KL-Loss   32.3401, KL-Weight  0.004\n",
      "TRAIN Batch 0050/267, Loss   68.8048, NLL-Loss   68.6635, KL-Loss   33.2077, KL-Weight  0.004\n",
      "TRAIN Batch 0100/267, Loss   69.2778, NLL-Loss   69.1142, KL-Loss   33.9436, KL-Weight  0.005\n",
      "TRAIN Batch 0150/267, Loss   69.8455, NLL-Loss   69.6906, KL-Loss   28.3781, KL-Weight  0.005\n",
      "TRAIN Batch 0200/267, Loss   69.5815, NLL-Loss   69.4141, KL-Loss   27.0653, KL-Weight  0.006\n",
      "TRAIN Batch 0250/267, Loss   66.7740, NLL-Loss   66.5969, KL-Loss   25.3032, KL-Weight  0.007\n",
      "TRAIN Batch 0267/267, Loss   71.2616, NLL-Loss   71.0861, KL-Loss   24.0482, KL-Weight  0.007\n",
      "TRAIN Epoch 02/10, Mean ELBO   75.5297\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E2.pytorch\n",
      " EPOCH 2, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   74.8630, NLL-Loss   74.6917, KL-Loss   23.4146, KL-Weight  0.007\n",
      "VAL Batch 0022/22, Loss   78.2426, NLL-Loss   78.0825, KL-Loss   21.8773, KL-Weight  0.007\n",
      "VAL Epoch 02/10, Mean ELBO   72.3105\n",
      " EPOCH 2, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   67.3598, NLL-Loss   67.1847, KL-Loss   23.9225, KL-Weight  0.007\n",
      "TEST Batch 0023/23, Loss   64.5306, NLL-Loss   64.3603, KL-Loss   23.2613, KL-Weight  0.007\n",
      "TEST Epoch 02/10, Mean ELBO   70.0638\n",
      "\n",
      "\n",
      " EPOCH 3, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   60.6573, NLL-Loss   60.4823, KL-Loss   23.9048, KL-Weight  0.007\n",
      "TRAIN Batch 0050/267, Loss   67.9742, NLL-Loss   67.7620, KL-Loss   25.6145, KL-Weight  0.008\n",
      "TRAIN Batch 0100/267, Loss   60.5795, NLL-Loss   60.3441, KL-Loss   25.0960, KL-Weight  0.009\n",
      "TRAIN Batch 0150/267, Loss   67.1035, NLL-Loss   66.8702, KL-Loss   21.9844, KL-Weight  0.011\n",
      "TRAIN Batch 0200/267, Loss   65.2411, NLL-Loss   64.9970, KL-Loss   20.3279, KL-Weight  0.012\n",
      "TRAIN Batch 0250/267, Loss   64.1251, NLL-Loss   63.8840, KL-Loss   17.7447, KL-Weight  0.014\n",
      "TRAIN Batch 0267/267, Loss   65.2948, NLL-Loss   65.0278, KL-Loss   18.8405, KL-Weight  0.014\n",
      "TRAIN Epoch 03/10, Mean ELBO   72.5490\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E3.pytorch\n",
      " EPOCH 3, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   74.8259, NLL-Loss   74.5600, KL-Loss   18.7219, KL-Weight  0.014\n",
      "VAL Batch 0022/22, Loss   78.5350, NLL-Loss   78.2927, KL-Loss   17.0549, KL-Weight  0.014\n",
      "VAL Epoch 03/10, Mean ELBO   71.5532\n",
      " EPOCH 3, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   66.6279, NLL-Loss   66.3588, KL-Loss   18.9511, KL-Weight  0.014\n",
      "TEST Batch 0023/23, Loss   65.1471, NLL-Loss   64.8848, KL-Loss   18.4697, KL-Weight  0.014\n",
      "TEST Epoch 03/10, Mean ELBO   69.3771\n",
      "\n",
      "\n",
      " EPOCH 4, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   70.3737, NLL-Loss   70.1078, KL-Loss   18.7232, KL-Weight  0.014\n",
      "TRAIN Batch 0050/267, Loss   63.6643, NLL-Loss   63.3859, KL-Loss   17.3284, KL-Weight  0.016\n",
      "TRAIN Batch 0100/267, Loss   67.9088, NLL-Loss   67.6137, KL-Loss   16.2494, KL-Weight  0.018\n",
      "TRAIN Batch 0150/267, Loss   63.4003, NLL-Loss   63.0947, KL-Loss   14.8866, KL-Weight  0.021\n",
      "TRAIN Batch 0200/267, Loss   65.1684, NLL-Loss   64.8513, KL-Loss   13.6676, KL-Weight  0.023\n",
      "TRAIN Batch 0250/267, Loss   63.5937, NLL-Loss   63.2401, KL-Loss   13.4908, KL-Weight  0.026\n",
      "TRAIN Batch 0267/267, Loss   64.9781, NLL-Loss   64.6459, KL-Loss   12.1600, KL-Weight  0.027\n",
      "TRAIN Epoch 04/10, Mean ELBO   70.4062\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E4.pytorch\n",
      " EPOCH 4, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   73.4943, NLL-Loss   73.1628, KL-Loss   12.1051, KL-Weight  0.027\n",
      "VAL Batch 0022/22, Loss   77.0169, NLL-Loss   76.7279, KL-Loss   10.5527, KL-Weight  0.027\n",
      "VAL Epoch 04/10, Mean ELBO   70.7734\n",
      " EPOCH 4, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   64.5993, NLL-Loss   64.2653, KL-Loss   12.1980, KL-Weight  0.027\n",
      "TEST Batch 0023/23, Loss   63.4305, NLL-Loss   63.1063, KL-Loss   11.8372, KL-Weight  0.027\n",
      "TEST Epoch 04/10, Mean ELBO   68.6130\n",
      "\n",
      "\n",
      " EPOCH 5, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   68.4294, NLL-Loss   68.0947, KL-Loss   12.2234, KL-Weight  0.027\n",
      "TRAIN Batch 0050/267, Loss   61.2824, NLL-Loss   60.9286, KL-Loss   11.4416, KL-Weight  0.031\n",
      "TRAIN Batch 0100/267, Loss   64.0099, NLL-Loss   63.6207, KL-Loss   11.1548, KL-Weight  0.035\n",
      "TRAIN Batch 0150/267, Loss   64.0379, NLL-Loss   63.6237, KL-Loss   10.5250, KL-Weight  0.039\n",
      "TRAIN Batch 0200/267, Loss   62.1528, NLL-Loss   61.7274, KL-Loss    9.5873, KL-Weight  0.044\n",
      "TRAIN Batch 0250/267, Loss   71.8206, NLL-Loss   71.3943, KL-Loss    8.5316, KL-Weight  0.050\n",
      "TRAIN Batch 0267/267, Loss   63.0246, NLL-Loss   62.5714, KL-Loss    8.7101, KL-Weight  0.052\n",
      "TRAIN Epoch 05/10, Mean ELBO   68.7210\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E5.pytorch\n",
      " EPOCH 5, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   73.7353, NLL-Loss   73.3121, KL-Loss    8.1134, KL-Weight  0.052\n",
      "VAL Batch 0022/22, Loss   76.7449, NLL-Loss   76.3929, KL-Loss    6.7492, KL-Weight  0.052\n",
      "VAL Epoch 05/10, Mean ELBO   70.2628\n",
      " EPOCH 5, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   65.3942, NLL-Loss   64.9598, KL-Loss    8.3306, KL-Weight  0.052\n",
      "TEST Batch 0023/23, Loss   63.4615, NLL-Loss   63.0412, KL-Loss    8.0582, KL-Weight  0.052\n",
      "TEST Epoch 05/10, Mean ELBO   68.1225\n",
      "\n",
      "\n",
      " EPOCH 6, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   66.0644, NLL-Loss   65.6372, KL-Loss    8.1914, KL-Weight  0.052\n",
      "TRAIN Batch 0050/267, Loss   59.9554, NLL-Loss   59.5140, KL-Loss    7.5205, KL-Weight  0.059\n",
      "TRAIN Batch 0100/267, Loss   63.1650, NLL-Loss   62.6494, KL-Loss    7.8135, KL-Weight  0.066\n",
      "TRAIN Batch 0150/267, Loss   63.5462, NLL-Loss   63.0685, KL-Loss    6.4450, KL-Weight  0.074\n",
      "TRAIN Batch 0200/267, Loss   60.3676, NLL-Loss   59.8627, KL-Loss    6.0705, KL-Weight  0.083\n",
      "TRAIN Batch 0250/267, Loss   58.4443, NLL-Loss   57.9573, KL-Loss    5.2251, KL-Weight  0.093\n",
      "TRAIN Batch 0267/267, Loss   62.9941, NLL-Loss   62.4290, KL-Loss    5.8332, KL-Weight  0.097\n",
      "TRAIN Epoch 06/10, Mean ELBO   67.3280\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E6.pytorch\n",
      " EPOCH 6, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   74.3040, NLL-Loss   73.7385, KL-Loss    5.8249, KL-Weight  0.097\n",
      "VAL Batch 0022/22, Loss   76.7841, NLL-Loss   76.3013, KL-Loss    4.9726, KL-Weight  0.097\n",
      "VAL Epoch 06/10, Mean ELBO   69.9427\n",
      " EPOCH 6, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   64.8252, NLL-Loss   64.2637, KL-Loss    5.7832, KL-Weight  0.097\n",
      "TEST Batch 0023/23, Loss   63.4871, NLL-Loss   62.9367, KL-Loss    5.6693, KL-Weight  0.097\n",
      "TEST Epoch 06/10, Mean ELBO   67.8063\n",
      "\n",
      "\n",
      " EPOCH 7, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   58.0372, NLL-Loss   57.4727, KL-Loss    5.8145, KL-Weight  0.097\n",
      "TRAIN Batch 0050/267, Loss   60.3827, NLL-Loss   59.7861, KL-Loss    5.4930, KL-Weight  0.109\n",
      "TRAIN Batch 0100/267, Loss   52.3408, NLL-Loss   51.7495, KL-Loss    4.8739, KL-Weight  0.121\n",
      "TRAIN Batch 0150/267, Loss   56.3255, NLL-Loss   55.7013, KL-Loss    4.6141, KL-Weight  0.135\n",
      "TRAIN Batch 0200/267, Loss   58.3015, NLL-Loss   57.7807, KL-Loss    3.4588, KL-Weight  0.151\n",
      "TRAIN Batch 0250/267, Loss   57.3182, NLL-Loss   56.7133, KL-Loss    3.6159, KL-Weight  0.167\n",
      "TRAIN Batch 0267/267, Loss   58.7264, NLL-Loss   58.1154, KL-Loss    3.5261, KL-Weight  0.173\n",
      "TRAIN Epoch 07/10, Mean ELBO   66.1506\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E7.pytorch\n",
      " EPOCH 7, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   74.2223, NLL-Loss   73.6621, KL-Loss    3.2260, KL-Weight  0.174\n",
      "VAL Batch 0022/22, Loss   76.1747, NLL-Loss   75.7132, KL-Loss    2.6579, KL-Weight  0.174\n",
      "VAL Epoch 07/10, Mean ELBO   69.6830\n",
      " EPOCH 7, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   64.4441, NLL-Loss   63.8609, KL-Loss    3.3586, KL-Weight  0.174\n",
      "TEST Batch 0023/23, Loss   63.6360, NLL-Loss   63.0965, KL-Loss    3.1069, KL-Weight  0.174\n",
      "TEST Epoch 07/10, Mean ELBO   67.5464\n",
      "\n",
      "\n",
      " EPOCH 8, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   62.9527, NLL-Loss   62.3809, KL-Loss    3.2931, KL-Weight  0.174\n",
      "TRAIN Batch 0050/267, Loss   61.2469, NLL-Loss   60.5895, KL-Loss    3.4184, KL-Weight  0.192\n",
      "TRAIN Batch 0100/267, Loss   51.6818, NLL-Loss   51.0723, KL-Loss    2.8684, KL-Weight  0.212\n",
      "TRAIN Batch 0150/267, Loss   50.6917, NLL-Loss   50.0904, KL-Loss    2.5680, KL-Weight  0.234\n",
      "TRAIN Batch 0200/267, Loss   59.5170, NLL-Loss   58.8975, KL-Loss    2.4073, KL-Weight  0.257\n",
      "TRAIN Batch 0250/267, Loss   56.1152, NLL-Loss   55.4745, KL-Loss    2.2725, KL-Weight  0.282\n",
      "TRAIN Batch 0267/267, Loss   61.8205, NLL-Loss   61.1745, KL-Loss    2.2232, KL-Weight  0.291\n",
      "TRAIN Epoch 08/10, Mean ELBO   65.1104\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E8.pytorch\n",
      " EPOCH 8, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   74.5622, NLL-Loss   73.9138, KL-Loss    2.2273, KL-Weight  0.291\n",
      "VAL Batch 0022/22, Loss   77.3035, NLL-Loss   76.7919, KL-Loss    1.7576, KL-Weight  0.291\n",
      "VAL Epoch 08/10, Mean ELBO   69.4980\n",
      " EPOCH 8, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   64.4650, NLL-Loss   63.8034, KL-Loss    2.2727, KL-Weight  0.291\n",
      "TEST Batch 0023/23, Loss   63.6906, NLL-Loss   63.0706, KL-Loss    2.1297, KL-Weight  0.291\n",
      "TEST Epoch 08/10, Mean ELBO   67.3567\n",
      "\n",
      "\n",
      " EPOCH 9, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   53.3317, NLL-Loss   52.6764, KL-Loss    2.2510, KL-Weight  0.291\n",
      "TRAIN Batch 0050/267, Loss   53.7738, NLL-Loss   53.1096, KL-Loss    2.0916, KL-Weight  0.318\n",
      "TRAIN Batch 0100/267, Loss   59.4809, NLL-Loss   58.7833, KL-Loss    2.0206, KL-Weight  0.345\n",
      "TRAIN Batch 0150/267, Loss   52.8730, NLL-Loss   52.1875, KL-Loss    1.8329, KL-Weight  0.374\n",
      "TRAIN Batch 0200/267, Loss   54.7317, NLL-Loss   54.0299, KL-Loss    1.7383, KL-Weight  0.404\n",
      "TRAIN Batch 0250/267, Loss   56.4447, NLL-Loss   55.7821, KL-Loss    1.5263, KL-Weight  0.434\n",
      "TRAIN Batch 0267/267, Loss   61.9359, NLL-Loss   61.3824, KL-Loss    1.2451, KL-Weight  0.445\n",
      "TRAIN Epoch 09/10, Mean ELBO   64.1886\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E9.pytorch\n",
      " EPOCH 9, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   74.9903, NLL-Loss   74.4064, KL-Loss    1.3114, KL-Weight  0.445\n",
      "VAL Batch 0022/22, Loss   77.7479, NLL-Loss   77.2987, KL-Loss    1.0089, KL-Weight  0.445\n",
      "VAL Epoch 09/10, Mean ELBO   69.3795\n",
      " EPOCH 9, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   64.3911, NLL-Loss   63.7795, KL-Loss    1.3737, KL-Weight  0.445\n",
      "TEST Batch 0023/23, Loss   63.7310, NLL-Loss   63.1491, KL-Loss    1.3070, KL-Weight  0.445\n",
      "TEST Epoch 09/10, Mean ELBO   67.2218\n",
      "\n",
      "\n",
      " EPOCH 10, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   49.2346, NLL-Loss   48.5559, KL-Loss    1.5243, KL-Weight  0.445\n",
      "TRAIN Batch 0050/267, Loss   56.2525, NLL-Loss   55.5609, KL-Loss    1.4521, KL-Weight  0.476\n",
      "TRAIN Batch 0100/267, Loss   53.6375, NLL-Loss   52.8688, KL-Loss    1.5147, KL-Weight  0.507\n",
      "TRAIN Batch 0150/267, Loss   56.8636, NLL-Loss   55.9802, KL-Loss    1.6400, KL-Weight  0.539\n",
      "TRAIN Batch 0200/267, Loss   58.0900, NLL-Loss   57.3540, KL-Loss    1.2923, KL-Weight  0.570\n",
      "TRAIN Batch 0250/267, Loss   57.8332, NLL-Loss   57.0308, KL-Loss    1.3376, KL-Weight  0.600\n",
      "TRAIN Batch 0267/267, Loss   56.1116, NLL-Loss   55.3820, KL-Loss    1.1960, KL-Weight  0.610\n",
      "TRAIN Epoch 10/10, Mean ELBO   63.3431\n",
      "Model saved at bin/2019-Feb-01-14:24:19/E10.pytorch\n",
      " EPOCH 10, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   74.7056, NLL-Loss   73.9755, KL-Loss    1.1956, KL-Weight  0.611\n",
      "VAL Batch 0022/22, Loss   79.4450, NLL-Loss   78.8427, KL-Loss    0.9863, KL-Weight  0.611\n",
      "VAL Epoch 10/10, Mean ELBO   69.3051\n",
      " EPOCH 10, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   64.2057, NLL-Loss   63.4391, KL-Loss    1.2553, KL-Weight  0.611\n",
      "TEST Batch 0023/23, Loss   64.2408, NLL-Loss   63.4984, KL-Loss    1.2158, KL-Weight  0.611\n",
      "TEST Epoch 10/10, Mean ELBO   67.1282\n",
      "-------------------------------------------\n",
      "\n",
      "Training Complete\n",
      "sending model to cuda\n",
      "loading cuda state\n",
      "Model loaded from bin/2019-Feb-01-14:24:19/E9.pytorch\n",
      "wrote samples to 'bin/2019-Feb-01-14:24:19/samples'\n",
      "sending model to cuda\n",
      "SentenceVAE(\n",
      "  (embedding): Embedding(10009, 300)\n",
      "  (embedding_dropout): Dropout(p=0.5)\n",
      "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (hidden2mean): Linear(in_features=256, out_features=20, bias=True)\n",
      "  (hidden2logv): Linear(in_features=256, out_features=20, bias=True)\n",
      "  (latent2hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (outputs2vocab): Linear(in_features=256, out_features=10009, bias=True)\n",
      ")\n",
      "Beginning training at: 2019-Feb-01-14:25:30\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      " EPOCH 1, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss  130.6212, NLL-Loss  130.6202, KL-Loss    0.4959, KL-Weight  0.002\n",
      "TRAIN Batch 0050/267, Loss   88.2156, NLL-Loss   88.1828, KL-Loss   15.0185, KL-Weight  0.002\n",
      "TRAIN Batch 0100/267, Loss   82.5061, NLL-Loss   82.4022, KL-Loss   42.0257, KL-Weight  0.002\n",
      "TRAIN Batch 0150/267, Loss   70.8005, NLL-Loss   70.6502, KL-Loss   53.6554, KL-Weight  0.003\n",
      "TRAIN Batch 0200/267, Loss   68.2718, NLL-Loss   68.0601, KL-Loss   66.7088, KL-Weight  0.003\n",
      "TRAIN Batch 0250/267, Loss   70.4079, NLL-Loss   70.1345, KL-Loss   76.0952, KL-Weight  0.004\n",
      "TRAIN Batch 0267/267, Loss   69.1813, NLL-Loss   68.8891, KL-Loss   77.9397, KL-Weight  0.004\n",
      "TRAIN Epoch 01/10, Mean ELBO   78.3195\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E1.pytorch\n",
      " EPOCH 1, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   70.4993, NLL-Loss   70.2304, KL-Loss   71.5324, KL-Weight  0.004\n",
      "VAL Batch 0022/22, Loss   83.4711, NLL-Loss   83.2472, KL-Loss   59.5727, KL-Weight  0.004\n",
      "VAL Epoch 01/10, Mean ELBO   67.4580\n",
      " EPOCH 1, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   64.0893, NLL-Loss   63.8150, KL-Loss   73.0027, KL-Weight  0.004\n",
      "TEST Batch 0023/23, Loss   61.9380, NLL-Loss   61.6498, KL-Loss   76.6942, KL-Weight  0.004\n",
      "TEST Epoch 01/10, Mean ELBO   65.0167\n",
      "\n",
      "\n",
      " EPOCH 2, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   70.7524, NLL-Loss   70.4728, KL-Loss   74.4040, KL-Weight  0.004\n",
      "TRAIN Batch 0050/267, Loss   65.7507, NLL-Loss   65.3894, KL-Loss   84.8917, KL-Weight  0.004\n",
      "TRAIN Batch 0100/267, Loss   73.2637, NLL-Loss   72.8504, KL-Loss   85.7345, KL-Weight  0.005\n",
      "TRAIN Batch 0150/267, Loss   54.9895, NLL-Loss   54.5122, KL-Loss   87.4448, KL-Weight  0.005\n",
      "TRAIN Batch 0200/267, Loss   58.2266, NLL-Loss   57.7232, KL-Loss   81.4418, KL-Weight  0.006\n",
      "TRAIN Batch 0250/267, Loss   66.0016, NLL-Loss   65.4115, KL-Loss   84.3251, KL-Weight  0.007\n",
      "TRAIN Batch 0267/267, Loss   53.5536, NLL-Loss   52.9324, KL-Loss   85.0883, KL-Weight  0.007\n",
      "TRAIN Epoch 02/10, Mean ELBO   70.0321\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E2.pytorch\n",
      " EPOCH 2, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   64.8333, NLL-Loss   64.2632, KL-Loss   77.8873, KL-Weight  0.007\n",
      "VAL Batch 0022/22, Loss   70.8021, NLL-Loss   70.2408, KL-Loss   76.6895, KL-Weight  0.007\n",
      "VAL Epoch 02/10, Mean ELBO   63.9753\n",
      " EPOCH 2, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   56.7011, NLL-Loss   56.1038, KL-Loss   81.6259, KL-Weight  0.007\n",
      "TEST Batch 0023/23, Loss   55.6675, NLL-Loss   55.0655, KL-Loss   82.2541, KL-Weight  0.007\n",
      "TEST Epoch 02/10, Mean ELBO   61.7400\n",
      "\n",
      "\n",
      " EPOCH 3, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   52.2245, NLL-Loss   51.6096, KL-Loss   84.0148, KL-Weight  0.007\n",
      "TRAIN Batch 0050/267, Loss   59.3586, NLL-Loss   58.5998, KL-Loss   91.5956, KL-Weight  0.008\n",
      "TRAIN Batch 0100/267, Loss   58.0066, NLL-Loss   57.1772, KL-Loss   88.4410, KL-Weight  0.009\n",
      "TRAIN Batch 0150/267, Loss   52.0436, NLL-Loss   51.1222, KL-Loss   86.8233, KL-Weight  0.011\n",
      "TRAIN Batch 0200/267, Loss   54.2704, NLL-Loss   53.2374, KL-Loss   86.0162, KL-Weight  0.012\n",
      "TRAIN Batch 0250/267, Loss   55.3959, NLL-Loss   54.2730, KL-Loss   82.6501, KL-Weight  0.014\n",
      "TRAIN Batch 0267/267, Loss   49.0245, NLL-Loss   47.8734, KL-Loss   81.2469, KL-Weight  0.014\n",
      "TRAIN Epoch 03/10, Mean ELBO   64.8608\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E3.pytorch\n",
      " EPOCH 3, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   63.1645, NLL-Loss   62.0903, KL-Loss   75.6328, KL-Weight  0.014\n",
      "VAL Batch 0022/22, Loss   69.8176, NLL-Loss   68.7586, KL-Loss   74.5631, KL-Weight  0.014\n",
      "VAL Epoch 03/10, Mean ELBO   62.0494\n",
      " EPOCH 3, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   54.6379, NLL-Loss   53.5173, KL-Loss   78.8953, KL-Weight  0.014\n",
      "TEST Batch 0023/23, Loss   53.0479, NLL-Loss   51.9299, KL-Loss   78.7141, KL-Weight  0.014\n",
      "TEST Epoch 03/10, Mean ELBO   59.8203\n",
      "\n",
      "\n",
      " EPOCH 4, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   50.9982, NLL-Loss   49.8061, KL-Loss   83.9342, KL-Weight  0.014\n",
      "TRAIN Batch 0050/267, Loss   42.5924, NLL-Loss   41.1644, KL-Loss   88.8925, KL-Weight  0.016\n",
      "TRAIN Batch 0100/267, Loss   52.5413, NLL-Loss   50.9998, KL-Loss   84.8674, KL-Weight  0.018\n",
      "TRAIN Batch 0150/267, Loss   46.1707, NLL-Loss   44.5232, KL-Loss   80.2373, KL-Weight  0.021\n",
      "TRAIN Batch 0200/267, Loss   53.4520, NLL-Loss   51.7164, KL-Loss   74.7989, KL-Weight  0.023\n",
      "TRAIN Batch 0250/267, Loss   46.3196, NLL-Loss   44.3164, KL-Loss   76.4236, KL-Weight  0.026\n",
      "TRAIN Batch 0267/267, Loss   51.8977, NLL-Loss   49.8827, KL-Loss   73.7572, KL-Weight  0.027\n",
      "TRAIN Epoch 04/10, Mean ELBO   61.1661\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E4.pytorch\n",
      " EPOCH 4, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   62.8187, NLL-Loss   60.9784, KL-Loss   67.1998, KL-Weight  0.027\n",
      "VAL Batch 0022/22, Loss   67.4950, NLL-Loss   65.6880, KL-Loss   65.9862, KL-Weight  0.027\n",
      "VAL Epoch 04/10, Mean ELBO   60.9532\n",
      " EPOCH 4, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   53.5124, NLL-Loss   51.6057, KL-Loss   69.6251, KL-Weight  0.027\n",
      "TEST Batch 0023/23, Loss   52.8195, NLL-Loss   50.9295, KL-Loss   69.0153, KL-Weight  0.027\n",
      "TEST Epoch 04/10, Mean ELBO   58.7389\n",
      "\n",
      "\n",
      " EPOCH 5, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   45.2935, NLL-Loss   43.3321, KL-Loss   71.6212, KL-Weight  0.027\n",
      "TRAIN Batch 0050/267, Loss   47.0524, NLL-Loss   44.7702, KL-Loss   73.8166, KL-Weight  0.031\n",
      "TRAIN Batch 0100/267, Loss   48.3073, NLL-Loss   45.9542, KL-Loss   67.4384, KL-Weight  0.035\n",
      "TRAIN Batch 0150/267, Loss   47.4841, NLL-Loss   44.8365, KL-Loss   67.2757, KL-Weight  0.039\n",
      "TRAIN Batch 0200/267, Loss   52.4856, NLL-Loss   49.7133, KL-Loss   62.4938, KL-Weight  0.044\n",
      "TRAIN Batch 0250/267, Loss   47.7776, NLL-Loss   44.8391, KL-Loss   58.8018, KL-Weight  0.050\n",
      "TRAIN Batch 0267/267, Loss   53.6903, NLL-Loss   50.5987, KL-Loss   59.4197, KL-Weight  0.052\n",
      "TRAIN Epoch 05/10, Mean ELBO   58.3930\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E5.pytorch\n",
      " EPOCH 5, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   63.8715, NLL-Loss   60.9254, KL-Loss   56.4901, KL-Weight  0.052\n",
      "VAL Batch 0022/22, Loss   73.5184, NLL-Loss   70.5505, KL-Loss   56.9060, KL-Weight  0.052\n",
      "VAL Epoch 05/10, Mean ELBO   60.5240\n",
      " EPOCH 5, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   54.3922, NLL-Loss   51.3354, KL-Loss   58.6123, KL-Weight  0.052\n",
      "TEST Batch 0023/23, Loss   54.1091, NLL-Loss   51.1138, KL-Loss   57.4326, KL-Weight  0.052\n",
      "TEST Epoch 05/10, Mean ELBO   58.2713\n",
      "\n",
      "\n",
      " EPOCH 6, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   46.9067, NLL-Loss   43.6834, KL-Loss   61.8030, KL-Weight  0.052\n",
      "TRAIN Batch 0050/267, Loss   46.2248, NLL-Loss   42.6558, KL-Loss   60.8109, KL-Weight  0.059\n",
      "TRAIN Batch 0100/267, Loss   42.7682, NLL-Loss   38.9361, KL-Loss   58.0717, KL-Weight  0.066\n",
      "TRAIN Batch 0150/267, Loss   46.0731, NLL-Loss   42.2810, KL-Loss   51.1590, KL-Weight  0.074\n",
      "TRAIN Batch 0200/267, Loss   41.7428, NLL-Loss   37.6645, KL-Loss   49.0344, KL-Weight  0.083\n",
      "TRAIN Batch 0250/267, Loss   49.1285, NLL-Loss   44.7009, KL-Loss   47.4995, KL-Weight  0.093\n",
      "TRAIN Batch 0267/267, Loss   43.4326, NLL-Loss   39.1484, KL-Loss   44.2269, KL-Weight  0.097\n",
      "TRAIN Epoch 06/10, Mean ELBO   56.3031\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E6.pytorch\n",
      " EPOCH 6, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   64.7440, NLL-Loss   60.4981, KL-Loss   43.7323, KL-Weight  0.097\n",
      "VAL Batch 0022/22, Loss   72.7914, NLL-Loss   68.6713, KL-Loss   42.4362, KL-Weight  0.097\n",
      "VAL Epoch 06/10, Mean ELBO   60.3438\n",
      " EPOCH 6, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   53.9365, NLL-Loss   49.6093, KL-Loss   44.5692, KL-Weight  0.097\n",
      "TEST Batch 0023/23, Loss   55.4084, NLL-Loss   51.1736, KL-Loss   43.6186, KL-Weight  0.097\n",
      "TEST Epoch 06/10, Mean ELBO   58.0808\n",
      "\n",
      "\n",
      " EPOCH 7, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   47.5499, NLL-Loss   43.1019, KL-Loss   45.8144, KL-Weight  0.097\n",
      "TRAIN Batch 0050/267, Loss   40.6298, NLL-Loss   35.7973, KL-Loss   44.4937, KL-Weight  0.109\n",
      "TRAIN Batch 0100/267, Loss   49.8030, NLL-Loss   44.5311, KL-Loss   43.4550, KL-Weight  0.121\n",
      "TRAIN Batch 0150/267, Loss   49.2134, NLL-Loss   43.8405, KL-Loss   39.7151, KL-Weight  0.135\n",
      "TRAIN Batch 0200/267, Loss   46.6053, NLL-Loss   40.7290, KL-Loss   39.0223, KL-Weight  0.151\n",
      "TRAIN Batch 0250/267, Loss   46.6616, NLL-Loss   40.7983, KL-Loss   35.0503, KL-Weight  0.167\n",
      "TRAIN Batch 0267/267, Loss   49.8390, NLL-Loss   43.4983, KL-Loss   36.5906, KL-Weight  0.173\n",
      "TRAIN Epoch 07/10, Mean ELBO   54.7792\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E7.pytorch\n",
      " EPOCH 7, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   66.6580, NLL-Loss   60.5769, KL-Loss   35.0198, KL-Weight  0.174\n",
      "VAL Batch 0022/22, Loss   74.4827, NLL-Loss   68.3174, KL-Loss   35.5047, KL-Weight  0.174\n",
      "VAL Epoch 07/10, Mean ELBO   60.5217\n",
      " EPOCH 7, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   56.4885, NLL-Loss   50.3630, KL-Loss   35.2762, KL-Weight  0.174\n",
      "TEST Batch 0023/23, Loss   57.4905, NLL-Loss   51.5547, KL-Loss   34.1832, KL-Weight  0.174\n",
      "TEST Epoch 07/10, Mean ELBO   58.2383\n",
      "\n",
      "\n",
      " EPOCH 8, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   44.2768, NLL-Loss   38.2830, KL-Loss   34.5175, KL-Weight  0.174\n",
      "TRAIN Batch 0050/267, Loss   44.5446, NLL-Loss   37.7466, KL-Loss   35.3471, KL-Weight  0.192\n",
      "TRAIN Batch 0100/267, Loss   47.9710, NLL-Loss   41.0339, KL-Loss   32.6471, KL-Weight  0.212\n",
      "TRAIN Batch 0150/267, Loss   48.1062, NLL-Loss   40.7195, KL-Loss   31.5465, KL-Weight  0.234\n",
      "TRAIN Batch 0200/267, Loss   47.5701, NLL-Loss   40.4027, KL-Loss   27.8553, KL-Weight  0.257\n",
      "TRAIN Batch 0250/267, Loss   47.9629, NLL-Loss   40.1455, KL-Loss   27.7297, KL-Weight  0.282\n",
      "TRAIN Batch 0267/267, Loss   51.4057, NLL-Loss   43.1961, KL-Loss   28.2513, KL-Weight  0.291\n",
      "TRAIN Epoch 08/10, Mean ELBO   53.7529\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E8.pytorch\n",
      " EPOCH 8, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   70.0016, NLL-Loss   62.1696, KL-Loss   26.9041, KL-Weight  0.291\n",
      "VAL Batch 0022/22, Loss   79.3185, NLL-Loss   71.6074, KL-Loss   26.4887, KL-Weight  0.291\n",
      "VAL Epoch 08/10, Mean ELBO   60.9666\n",
      " EPOCH 8, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   59.0615, NLL-Loss   51.2627, KL-Loss   26.7899, KL-Weight  0.291\n",
      "TEST Batch 0023/23, Loss   60.3472, NLL-Loss   52.7219, KL-Loss   26.1939, KL-Weight  0.291\n",
      "TEST Epoch 08/10, Mean ELBO   58.6481\n",
      "\n",
      "\n",
      " EPOCH 9, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   43.9943, NLL-Loss   36.0520, KL-Loss   27.2829, KL-Weight  0.291\n",
      "TRAIN Batch 0050/267, Loss   50.4950, NLL-Loss   41.8265, KL-Loss   27.2971, KL-Weight  0.318\n",
      "TRAIN Batch 0100/267, Loss   49.7816, NLL-Loss   40.9725, KL-Loss   25.5155, KL-Weight  0.345\n",
      "TRAIN Batch 0150/267, Loss   51.5202, NLL-Loss   42.4442, KL-Loss   24.2661, KL-Weight  0.374\n",
      "TRAIN Batch 0200/267, Loss   47.8297, NLL-Loss   38.5763, KL-Loss   22.9204, KL-Weight  0.404\n",
      "TRAIN Batch 0250/267, Loss   50.7393, NLL-Loss   41.3454, KL-Loss   21.6382, KL-Weight  0.434\n",
      "TRAIN Batch 0267/267, Loss   47.4295, NLL-Loss   38.6895, KL-Loss   19.6579, KL-Weight  0.445\n",
      "TRAIN Epoch 09/10, Mean ELBO   53.1420\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E9.pytorch\n",
      " EPOCH 9, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   72.6564, NLL-Loss   63.2582, KL-Loss   21.1090, KL-Weight  0.445\n",
      "VAL Batch 0022/22, Loss   81.5280, NLL-Loss   72.1662, KL-Loss   21.0273, KL-Weight  0.445\n",
      "VAL Epoch 09/10, Mean ELBO   61.6283\n",
      " EPOCH 9, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   61.4684, NLL-Loss   52.0695, KL-Loss   21.1106, KL-Weight  0.445\n",
      "TEST Batch 0023/23, Loss   62.6642, NLL-Loss   53.6290, KL-Loss   20.2938, KL-Weight  0.445\n",
      "TEST Epoch 09/10, Mean ELBO   59.2747\n",
      "\n",
      "\n",
      " EPOCH 10, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/267, Loss   46.6880, NLL-Loss   37.0458, KL-Loss   21.6573, KL-Weight  0.445\n",
      "TRAIN Batch 0050/267, Loss   51.2688, NLL-Loss   41.0237, KL-Loss   21.5112, KL-Weight  0.476\n",
      "TRAIN Batch 0100/267, Loss   55.1252, NLL-Loss   44.9915, KL-Loss   19.9680, KL-Weight  0.507\n",
      "TRAIN Batch 0150/267, Loss   52.0718, NLL-Loss   41.7606, KL-Loss   19.1419, KL-Weight  0.539\n",
      "TRAIN Batch 0200/267, Loss   44.5937, NLL-Loss   34.6242, KL-Loss   17.5042, KL-Weight  0.570\n",
      "TRAIN Batch 0250/267, Loss   49.9716, NLL-Loss   39.5024, KL-Loss   17.4520, KL-Weight  0.600\n",
      "TRAIN Batch 0267/267, Loss   50.5881, NLL-Loss   40.4048, KL-Loss   16.6927, KL-Weight  0.610\n",
      "TRAIN Epoch 10/10, Mean ELBO   52.8396\n",
      "Model saved at bin/2019-Feb-01-14:25:30/E10.pytorch\n",
      " EPOCH 10, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/22, Loss   74.3561, NLL-Loss   64.2085, KL-Loss   16.6180, KL-Weight  0.611\n",
      "VAL Batch 0022/22, Loss   87.0032, NLL-Loss   76.9371, KL-Loss   16.4845, KL-Weight  0.611\n",
      "VAL Epoch 10/10, Mean ELBO   62.4078\n",
      " EPOCH 10, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/23, Loss   64.9593, NLL-Loss   54.6984, KL-Loss   16.8036, KL-Weight  0.611\n",
      "TEST Batch 0023/23, Loss   65.4408, NLL-Loss   55.5628, KL-Loss   16.1765, KL-Weight  0.611\n",
      "TEST Epoch 10/10, Mean ELBO   60.0153\n",
      "-------------------------------------------\n",
      "\n",
      "Training Complete\n",
      "sending model to cuda\n",
      "loading cuda state\n",
      "Model loaded from bin/2019-Feb-01-14:25:30/E9.pytorch\n",
      "wrote samples to 'bin/2019-Feb-01-14:25:30/samples'\n",
      "Preprocessing Penn Treebank *train* data:\n",
      "------------------------------------------\n",
      "Creating vocab file ...\n",
      "\t[Getting word counts]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e0ff0989d44b478b6e5fe57d068908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t[Creating dictionaries]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a5e564cee4ffa91ead8b80231f4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10005), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t[Loading pretrained GLOVE embeddings -- this may take a while the first time]\n",
      "Loaded 400000 words\n",
      "\t[Mapping vocab to GLOVE embeddings]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853876bfce114771a3a9ee0e12658546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10009), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary created (10009 word types)!\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77bac14298b4ad080cc970e815479b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset created, with:\n",
      "\t35528 sentences\n",
      "\t736759 word tokens\n",
      "\t20.737418374240036 avg. sentence length\n",
      "\n",
      "\n",
      "Preprocessing Penn Treebank *val* data:\n",
      "------------------------------------------\n",
      "Loading vocab file ...\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736c3525249848d5a11fa9d00da0428f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset created, with:\n",
      "\t2865 sentences\n",
      "\t58999 word tokens\n",
      "\t20.593019197207678 avg. sentence length\n",
      "\n",
      "\n",
      "Preprocessing Penn Treebank *test* data:\n",
      "------------------------------------------\n",
      "Loading vocab file ...\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcff71814a3467c85c58b6ab1dffa78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset created, with:\n",
      "\t3114 sentences\n",
      "\t63958 word tokens\n",
      "\t20.538856775850995 avg. sentence length\n",
      "\n",
      "\n",
      "sending model to cuda\n",
      "SentenceVAE(\n",
      "  (embedding): Embedding(10009, 300)\n",
      "  (embedding_dropout): Dropout(p=0.5)\n",
      "  (encoder_rnn): GRU(300, 256, batch_first=True, bidirectional=True)\n",
      "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (hidden2mean): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (hidden2logv): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (latent2hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (outputs2vocab): Linear(in_features=256, out_features=10009, bias=True)\n",
      ")\n",
      "Beginning training at: 2019-Feb-01-14:26:59\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      " EPOCH 1, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss  186.8918, NLL-Loss  186.8909, KL-Loss    0.4494, KL-Weight  0.002\n",
      "TRAIN Batch 0050/555, Loss  129.8413, NLL-Loss  129.7867, KL-Loss   25.0120, KL-Weight  0.002\n",
      "TRAIN Batch 0100/555, Loss  124.5163, NLL-Loss  124.4233, KL-Loss   37.6112, KL-Weight  0.002\n",
      "TRAIN Batch 0150/555, Loss  110.7685, NLL-Loss  110.6663, KL-Loss   36.4750, KL-Weight  0.003\n",
      "TRAIN Batch 0200/555, Loss  106.6206, NLL-Loss  106.4867, KL-Loss   42.2156, KL-Weight  0.003\n",
      "TRAIN Batch 0250/555, Loss  103.5644, NLL-Loss  103.4285, KL-Loss   37.8283, KL-Weight  0.004\n",
      "TRAIN Batch 0300/555, Loss  107.2320, NLL-Loss  107.0760, KL-Loss   38.3272, KL-Weight  0.004\n",
      "TRAIN Batch 0350/555, Loss  106.4661, NLL-Loss  106.3082, KL-Loss   34.2711, KL-Weight  0.005\n",
      "TRAIN Batch 0400/555, Loss  111.1269, NLL-Loss  110.9585, KL-Loss   32.2598, KL-Weight  0.005\n",
      "TRAIN Batch 0450/555, Loss  108.7478, NLL-Loss  108.5608, KL-Loss   31.6321, KL-Weight  0.006\n",
      "TRAIN Batch 0500/555, Loss  103.3181, NLL-Loss  103.1302, KL-Loss   28.0784, KL-Weight  0.007\n",
      "TRAIN Batch 0550/555, Loss  109.8255, NLL-Loss  109.6181, KL-Loss   27.3680, KL-Weight  0.008\n",
      "TRAIN Batch 0555/555, Loss  102.0952, NLL-Loss  101.8963, KL-Loss   25.9324, KL-Weight  0.008\n",
      "TRAIN Epoch 01/10, Mean ELBO  114.7362\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E1.pytorch\n",
      " EPOCH 1, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  118.2304, NLL-Loss  118.0276, KL-Loss   26.3732, KL-Weight  0.008\n",
      "VAL Batch 0044/44, Loss  119.2030, NLL-Loss  119.0011, KL-Loss   26.2610, KL-Weight  0.008\n",
      "VAL Epoch 01/10, Mean ELBO  104.9992\n",
      " EPOCH 1, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss  108.0376, NLL-Loss  107.8285, KL-Loss   27.1821, KL-Weight  0.008\n",
      "TEST Batch 0048/48, Loss  102.0831, NLL-Loss  101.8780, KL-Loss   26.6653, KL-Weight  0.008\n",
      "TEST Epoch 01/10, Mean ELBO  102.0989\n",
      "\n",
      "\n",
      " EPOCH 2, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss  100.9283, NLL-Loss  100.7228, KL-Loss   26.7244, KL-Weight  0.008\n",
      "TRAIN Batch 0050/555, Loss  103.0255, NLL-Loss  102.8083, KL-Loss   24.9476, KL-Weight  0.009\n",
      "TRAIN Batch 0100/555, Loss  101.0209, NLL-Loss  100.7499, KL-Loss   27.5009, KL-Weight  0.010\n",
      "TRAIN Batch 0150/555, Loss  105.4586, NLL-Loss  105.1949, KL-Loss   23.6451, KL-Weight  0.011\n",
      "TRAIN Batch 0200/555, Loss  103.9744, NLL-Loss  103.7110, KL-Loss   20.8715, KL-Weight  0.013\n",
      "TRAIN Batch 0250/555, Loss  108.9575, NLL-Loss  108.6667, KL-Loss   20.3768, KL-Weight  0.014\n",
      "TRAIN Batch 0300/555, Loss  101.3359, NLL-Loss  101.0338, KL-Loss   18.7154, KL-Weight  0.016\n",
      "TRAIN Batch 0350/555, Loss   88.3606, NLL-Loss   88.0696, KL-Loss   15.9431, KL-Weight  0.018\n",
      "TRAIN Batch 0400/555, Loss   97.8708, NLL-Loss   97.5298, KL-Loss   16.5250, KL-Weight  0.021\n",
      "TRAIN Batch 0450/555, Loss  101.9859, NLL-Loss  101.6723, KL-Loss   13.4487, KL-Weight  0.023\n",
      "TRAIN Batch 0500/555, Loss  101.7507, NLL-Loss  101.4196, KL-Loss   12.5693, KL-Weight  0.026\n",
      "TRAIN Batch 0550/555, Loss   98.6668, NLL-Loss   98.3438, KL-Loss   10.8590, KL-Weight  0.030\n",
      "TRAIN Batch 0555/555, Loss  107.6646, NLL-Loss  107.3325, KL-Loss   11.0328, KL-Weight  0.030\n",
      "TRAIN Epoch 02/10, Mean ELBO  108.4872\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E2.pytorch\n",
      " EPOCH 2, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  115.5001, NLL-Loss  115.1708, KL-Loss   10.9109, KL-Weight  0.030\n",
      "VAL Batch 0044/44, Loss  114.5747, NLL-Loss  114.2450, KL-Loss   10.9255, KL-Weight  0.030\n",
      "VAL Epoch 02/10, Mean ELBO  103.4471\n",
      " EPOCH 2, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss  103.7501, NLL-Loss  103.4024, KL-Loss   11.5213, KL-Weight  0.030\n",
      "TEST Batch 0048/48, Loss   99.2276, NLL-Loss   98.8924, KL-Loss   11.1075, KL-Weight  0.030\n",
      "TEST Epoch 02/10, Mean ELBO  100.5304\n",
      "\n",
      "\n",
      " EPOCH 3, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss   93.3866, NLL-Loss   93.0530, KL-Loss   11.0565, KL-Weight  0.030\n",
      "TRAIN Batch 0050/555, Loss  100.9976, NLL-Loss  100.5862, KL-Loss   12.0782, KL-Weight  0.034\n",
      "TRAIN Batch 0100/555, Loss  103.1128, NLL-Loss  102.7533, KL-Loss    9.3557, KL-Weight  0.038\n",
      "TRAIN Batch 0150/555, Loss   98.7015, NLL-Loss   98.3526, KL-Loss    8.0560, KL-Weight  0.043\n",
      "TRAIN Batch 0200/555, Loss   98.3456, NLL-Loss   97.9529, KL-Loss    8.0466, KL-Weight  0.049\n",
      "TRAIN Batch 0250/555, Loss   94.4808, NLL-Loss   94.1615, KL-Loss    5.8110, KL-Weight  0.055\n",
      "TRAIN Batch 0300/555, Loss   98.8799, NLL-Loss   98.5344, KL-Loss    5.5895, KL-Weight  0.062\n",
      "TRAIN Batch 0350/555, Loss  107.0369, NLL-Loss  106.6843, KL-Loss    5.0770, KL-Weight  0.069\n",
      "TRAIN Batch 0400/555, Loss  103.9421, NLL-Loss  103.6199, KL-Loss    4.1311, KL-Weight  0.078\n",
      "TRAIN Batch 0450/555, Loss   98.1810, NLL-Loss   97.8037, KL-Loss    4.3139, KL-Weight  0.087\n",
      "TRAIN Batch 0500/555, Loss  102.0041, NLL-Loss  101.6305, KL-Loss    3.8132, KL-Weight  0.098\n",
      "TRAIN Batch 0550/555, Loss   96.0907, NLL-Loss   95.7119, KL-Loss    3.4566, KL-Weight  0.110\n",
      "TRAIN Batch 0555/555, Loss  103.6134, NLL-Loss  103.2624, KL-Loss    3.1679, KL-Weight  0.111\n",
      "TRAIN Epoch 03/10, Mean ELBO  104.9059\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E3.pytorch\n",
      " EPOCH 3, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  112.8110, NLL-Loss  112.4512, KL-Loss    3.2395, KL-Weight  0.111\n",
      "VAL Batch 0044/44, Loss  111.9688, NLL-Loss  111.6393, KL-Loss    2.9663, KL-Weight  0.111\n",
      "VAL Epoch 03/10, Mean ELBO  101.9310\n",
      " EPOCH 3, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss  100.5305, NLL-Loss  100.1609, KL-Loss    3.3279, KL-Weight  0.111\n",
      "TEST Batch 0048/48, Loss   96.1324, NLL-Loss   95.7750, KL-Loss    3.2184, KL-Weight  0.111\n",
      "TEST Epoch 03/10, Mean ELBO   98.9931\n",
      "\n",
      "\n",
      " EPOCH 4, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss   90.5133, NLL-Loss   90.1411, KL-Loss    3.3522, KL-Weight  0.111\n",
      "TRAIN Batch 0050/555, Loss  102.3619, NLL-Loss  101.9499, KL-Loss    3.3227, KL-Weight  0.124\n",
      "TRAIN Batch 0100/555, Loss  110.4978, NLL-Loss  110.0830, KL-Loss    3.0007, KL-Weight  0.138\n",
      "TRAIN Batch 0150/555, Loss   96.4400, NLL-Loss   95.9922, KL-Loss    2.9115, KL-Weight  0.154\n",
      "TRAIN Batch 0200/555, Loss   99.0433, NLL-Loss   98.5738, KL-Loss    2.7490, KL-Weight  0.171\n",
      "TRAIN Batch 0250/555, Loss   99.3624, NLL-Loss   98.8865, KL-Loss    2.5147, KL-Weight  0.189\n",
      "TRAIN Batch 0300/555, Loss   88.6295, NLL-Loss   88.1514, KL-Loss    2.2856, KL-Weight  0.209\n",
      "TRAIN Batch 0350/555, Loss   88.0415, NLL-Loss   87.4606, KL-Loss    2.5194, KL-Weight  0.231\n",
      "TRAIN Batch 0400/555, Loss   95.3980, NLL-Loss   94.8352, KL-Loss    2.2200, KL-Weight  0.254\n",
      "TRAIN Batch 0450/555, Loss   87.8726, NLL-Loss   87.2480, KL-Loss    2.2477, KL-Weight  0.278\n",
      "TRAIN Batch 0500/555, Loss   95.3292, NLL-Loss   94.7137, KL-Loss    2.0270, KL-Weight  0.304\n",
      "TRAIN Batch 0550/555, Loss   95.1172, NLL-Loss   94.5027, KL-Loss    1.8582, KL-Weight  0.331\n",
      "TRAIN Batch 0555/555, Loss  105.2441, NLL-Loss  104.6584, KL-Loss    1.7563, KL-Weight  0.333\n",
      "TRAIN Epoch 04/10, Mean ELBO  102.3719\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E4.pytorch\n",
      " EPOCH 4, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  112.5120, NLL-Loss  111.8866, KL-Loss    1.8723, KL-Weight  0.334\n",
      "VAL Batch 0044/44, Loss  111.2946, NLL-Loss  110.7076, KL-Loss    1.7573, KL-Weight  0.334\n",
      "VAL Epoch 04/10, Mean ELBO  101.0016\n",
      " EPOCH 4, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss   98.6423, NLL-Loss   97.9887, KL-Loss    1.9568, KL-Weight  0.334\n",
      "TEST Batch 0048/48, Loss   95.4833, NLL-Loss   94.8689, KL-Loss    1.8392, KL-Weight  0.334\n",
      "TEST Epoch 04/10, Mean ELBO   98.0563\n",
      "\n",
      "\n",
      " EPOCH 5, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss   97.0573, NLL-Loss   96.3990, KL-Loss    1.9709, KL-Weight  0.334\n",
      "TRAIN Batch 0050/555, Loss   88.4291, NLL-Loss   87.7807, KL-Loss    1.7891, KL-Weight  0.362\n",
      "TRAIN Batch 0100/555, Loss   88.5060, NLL-Loss   87.7679, KL-Loss    1.8841, KL-Weight  0.392\n",
      "TRAIN Batch 0150/555, Loss   88.4450, NLL-Loss   87.7646, KL-Loss    1.6129, KL-Weight  0.422\n",
      "TRAIN Batch 0200/555, Loss   89.1937, NLL-Loss   88.4149, KL-Loss    1.7206, KL-Weight  0.453\n",
      "TRAIN Batch 0250/555, Loss   94.2903, NLL-Loss   93.4968, KL-Loss    1.6402, KL-Weight  0.484\n",
      "TRAIN Batch 0300/555, Loss   94.7318, NLL-Loss   93.9315, KL-Loss    1.5540, KL-Weight  0.515\n",
      "TRAIN Batch 0350/555, Loss   95.1489, NLL-Loss   94.3651, KL-Loss    1.4353, KL-Weight  0.546\n",
      "TRAIN Batch 0400/555, Loss   92.9311, NLL-Loss   92.2013, KL-Loss    1.2651, KL-Weight  0.577\n",
      "TRAIN Batch 0450/555, Loss   90.1928, NLL-Loss   89.4132, KL-Loss    1.2842, KL-Weight  0.607\n",
      "TRAIN Batch 0500/555, Loss   89.1568, NLL-Loss   88.3311, KL-Loss    1.2973, KL-Weight  0.636\n",
      "TRAIN Batch 0550/555, Loss   94.3847, NLL-Loss   93.4874, KL-Loss    1.3496, KL-Weight  0.665\n",
      "TRAIN Batch 0555/555, Loss   98.8336, NLL-Loss   98.0790, KL-Loss    1.1302, KL-Weight  0.668\n",
      "TRAIN Epoch 05/10, Mean ELBO  100.4568\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E5.pytorch\n",
      " EPOCH 5, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  111.4745, NLL-Loss  110.6669, KL-Loss    1.2086, KL-Weight  0.668\n",
      "VAL Batch 0044/44, Loss  111.0348, NLL-Loss  110.2515, KL-Loss    1.1722, KL-Weight  0.668\n",
      "VAL Epoch 05/10, Mean ELBO  100.2941\n",
      " EPOCH 5, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss   97.5285, NLL-Loss   96.6978, KL-Loss    1.2432, KL-Weight  0.668\n",
      "TEST Batch 0048/48, Loss   94.8401, NLL-Loss   94.0205, KL-Loss    1.2266, KL-Weight  0.668\n",
      "TEST Epoch 05/10, Mean ELBO   97.3479\n",
      "\n",
      "\n",
      " EPOCH 6, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss   79.6598, NLL-Loss   78.8308, KL-Loss    1.2407, KL-Weight  0.668\n",
      "TRAIN Batch 0050/555, Loss   86.5169, NLL-Loss   85.6567, KL-Loss    1.2371, KL-Weight  0.695\n",
      "TRAIN Batch 0100/555, Loss   89.4453, NLL-Loss   88.5702, KL-Loss    1.2135, KL-Weight  0.721\n",
      "TRAIN Batch 0150/555, Loss   94.1278, NLL-Loss   93.3461, KL-Loss    1.0485, KL-Weight  0.746\n",
      "TRAIN Batch 0200/555, Loss   91.4432, NLL-Loss   90.6610, KL-Loss    1.0178, KL-Weight  0.769\n",
      "TRAIN Batch 0250/555, Loss   96.4351, NLL-Loss   95.6898, KL-Loss    0.9434, KL-Weight  0.790\n",
      "TRAIN Batch 0300/555, Loss   89.9455, NLL-Loss   89.0683, KL-Loss    1.0830, KL-Weight  0.810\n",
      "TRAIN Batch 0350/555, Loss   94.6983, NLL-Loss   93.8233, KL-Loss    1.0561, KL-Weight  0.828\n",
      "TRAIN Batch 0400/555, Loss   98.1309, NLL-Loss   97.3929, KL-Loss    0.8728, KL-Weight  0.846\n",
      "TRAIN Batch 0450/555, Loss   94.2935, NLL-Loss   93.5288, KL-Loss    0.8880, KL-Weight  0.861\n",
      "TRAIN Batch 0500/555, Loss   97.5322, NLL-Loss   96.7683, KL-Loss    0.8725, KL-Weight  0.875\n",
      "TRAIN Batch 0550/555, Loss   89.8082, NLL-Loss   89.0670, KL-Loss    0.8343, KL-Weight  0.888\n",
      "TRAIN Batch 0555/555, Loss   56.6872, NLL-Loss   55.8287, KL-Loss    0.9649, KL-Weight  0.890\n",
      "TRAIN Epoch 06/10, Mean ELBO   98.8998\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E6.pytorch\n",
      " EPOCH 6, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  111.8900, NLL-Loss  111.1132, KL-Loss    0.8729, KL-Weight  0.890\n",
      "VAL Batch 0044/44, Loss  110.9327, NLL-Loss  110.2120, KL-Loss    0.8098, KL-Weight  0.890\n",
      "VAL Epoch 06/10, Mean ELBO   99.7656\n",
      " EPOCH 6, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss   97.1418, NLL-Loss   96.3298, KL-Loss    0.9125, KL-Weight  0.890\n",
      "TEST Batch 0048/48, Loss   94.3953, NLL-Loss   93.6145, KL-Loss    0.8774, KL-Weight  0.890\n",
      "TEST Epoch 06/10, Mean ELBO   96.8082\n",
      "\n",
      "\n",
      " EPOCH 7, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss   86.5761, NLL-Loss   85.8374, KL-Loss    0.8300, KL-Weight  0.890\n",
      "TRAIN Batch 0050/555, Loss   90.6988, NLL-Loss   89.9584, KL-Loss    0.8212, KL-Weight  0.902\n",
      "TRAIN Batch 0100/555, Loss   89.8694, NLL-Loss   89.1203, KL-Loss    0.8213, KL-Weight  0.912\n",
      "TRAIN Batch 0150/555, Loss   90.1049, NLL-Loss   89.3759, KL-Loss    0.7909, KL-Weight  0.922\n",
      "TRAIN Batch 0200/555, Loss   90.5101, NLL-Loss   89.7310, KL-Loss    0.8376, KL-Weight  0.930\n",
      "TRAIN Batch 0250/555, Loss   86.2140, NLL-Loss   85.5008, KL-Loss    0.7605, KL-Weight  0.938\n",
      "TRAIN Batch 0300/555, Loss   81.2188, NLL-Loss   80.4287, KL-Loss    0.8363, KL-Weight  0.945\n",
      "TRAIN Batch 0350/555, Loss   90.5085, NLL-Loss   89.7808, KL-Loss    0.7652, KL-Weight  0.951\n",
      "TRAIN Batch 0400/555, Loss   92.0979, NLL-Loss   91.4226, KL-Loss    0.7060, KL-Weight  0.956\n",
      "TRAIN Batch 0450/555, Loss   88.7221, NLL-Loss   88.0037, KL-Loss    0.7472, KL-Weight  0.961\n",
      "TRAIN Batch 0500/555, Loss   87.5375, NLL-Loss   86.8555, KL-Loss    0.7061, KL-Weight  0.966\n",
      "TRAIN Batch 0550/555, Loss   87.7068, NLL-Loss   86.9643, KL-Loss    0.7656, KL-Weight  0.970\n",
      "TRAIN Batch 0555/555, Loss   84.7761, NLL-Loss   84.0855, KL-Loss    0.7119, KL-Weight  0.970\n",
      "TRAIN Epoch 07/10, Mean ELBO   97.5719\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E7.pytorch\n",
      " EPOCH 7, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  111.5717, NLL-Loss  110.8624, KL-Loss    0.7312, KL-Weight  0.970\n",
      "VAL Batch 0044/44, Loss  111.1219, NLL-Loss  110.4704, KL-Loss    0.6716, KL-Weight  0.970\n",
      "VAL Epoch 07/10, Mean ELBO   99.3505\n",
      " EPOCH 7, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss   96.1774, NLL-Loss   95.4459, KL-Loss    0.7540, KL-Weight  0.970\n",
      "TEST Batch 0048/48, Loss   93.7703, NLL-Loss   93.0781, KL-Loss    0.7135, KL-Weight  0.970\n",
      "TEST Epoch 07/10, Mean ELBO   96.3808\n",
      "\n",
      "\n",
      " EPOCH 8, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss   84.9926, NLL-Loss   84.2614, KL-Loss    0.7537, KL-Weight  0.970\n",
      "TRAIN Batch 0050/555, Loss   87.6429, NLL-Loss   86.9023, KL-Loss    0.7607, KL-Weight  0.974\n",
      "TRAIN Batch 0100/555, Loss   91.2148, NLL-Loss   90.4668, KL-Loss    0.7659, KL-Weight  0.977\n",
      "TRAIN Batch 0150/555, Loss   96.6906, NLL-Loss   96.0229, KL-Loss    0.6818, KL-Weight  0.979\n",
      "TRAIN Batch 0200/555, Loss   95.0959, NLL-Loss   94.4479, KL-Loss    0.6600, KL-Weight  0.982\n",
      "TRAIN Batch 0250/555, Loss   97.5956, NLL-Loss   96.8769, KL-Loss    0.7305, KL-Weight  0.984\n",
      "TRAIN Batch 0300/555, Loss   91.1575, NLL-Loss   90.4344, KL-Loss    0.7336, KL-Weight  0.986\n",
      "TRAIN Batch 0350/555, Loss   82.6377, NLL-Loss   81.9781, KL-Loss    0.6680, KL-Weight  0.987\n",
      "TRAIN Batch 0400/555, Loss   91.2695, NLL-Loss   90.6236, KL-Loss    0.6532, KL-Weight  0.989\n",
      "TRAIN Batch 0450/555, Loss   85.7525, NLL-Loss   85.0028, KL-Loss    0.7572, KL-Weight  0.990\n",
      "TRAIN Batch 0500/555, Loss   89.2185, NLL-Loss   88.5071, KL-Loss    0.7176, KL-Weight  0.991\n",
      "TRAIN Batch 0550/555, Loss   87.3792, NLL-Loss   86.7500, KL-Loss    0.6340, KL-Weight  0.992\n",
      "TRAIN Batch 0555/555, Loss   84.6482, NLL-Loss   83.8789, KL-Loss    0.7753, KL-Weight  0.992\n",
      "TRAIN Epoch 08/10, Mean ELBO   96.3961\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E8.pytorch\n",
      " EPOCH 8, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  112.2423, NLL-Loss  111.6175, KL-Loss    0.6296, KL-Weight  0.992\n",
      "VAL Batch 0044/44, Loss  110.3606, NLL-Loss  109.7778, KL-Loss    0.5874, KL-Weight  0.992\n",
      "VAL Epoch 08/10, Mean ELBO   99.0125\n",
      " EPOCH 8, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss   96.0322, NLL-Loss   95.3639, KL-Loss    0.6734, KL-Weight  0.992\n",
      "TEST Batch 0048/48, Loss   93.2804, NLL-Loss   92.6457, KL-Loss    0.6396, KL-Weight  0.992\n",
      "TEST Epoch 08/10, Mean ELBO   96.0340\n",
      "\n",
      "\n",
      " EPOCH 9, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss   85.4534, NLL-Loss   84.7898, KL-Loss    0.6686, KL-Weight  0.992\n",
      "TRAIN Batch 0050/555, Loss   83.7007, NLL-Loss   82.9612, KL-Loss    0.7445, KL-Weight  0.993\n",
      "TRAIN Batch 0100/555, Loss   81.6223, NLL-Loss   80.8783, KL-Loss    0.7485, KL-Weight  0.994\n",
      "TRAIN Batch 0150/555, Loss   90.1328, NLL-Loss   89.3518, KL-Loss    0.7851, KL-Weight  0.995\n",
      "TRAIN Batch 0200/555, Loss   94.0766, NLL-Loss   93.4068, KL-Loss    0.6729, KL-Weight  0.995\n",
      "TRAIN Batch 0250/555, Loss   89.2781, NLL-Loss   88.6182, KL-Loss    0.6626, KL-Weight  0.996\n",
      "TRAIN Batch 0300/555, Loss   88.8642, NLL-Loss   88.1713, KL-Loss    0.6953, KL-Weight  0.996\n",
      "TRAIN Batch 0350/555, Loss   90.4357, NLL-Loss   89.6694, KL-Loss    0.7687, KL-Weight  0.997\n",
      "TRAIN Batch 0400/555, Loss   82.8992, NLL-Loss   82.3078, KL-Loss    0.5931, KL-Weight  0.997\n",
      "TRAIN Batch 0450/555, Loss   90.3498, NLL-Loss   89.7924, KL-Loss    0.5588, KL-Weight  0.998\n",
      "TRAIN Batch 0500/555, Loss   87.6176, NLL-Loss   87.0032, KL-Loss    0.6158, KL-Weight  0.998\n",
      "TRAIN Batch 0550/555, Loss   92.8778, NLL-Loss   92.2593, KL-Loss    0.6196, KL-Weight  0.998\n",
      "TRAIN Batch 0555/555, Loss   88.5159, NLL-Loss   87.9461, KL-Loss    0.5709, KL-Weight  0.998\n",
      "TRAIN Epoch 09/10, Mean ELBO   95.3444\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E9.pytorch\n",
      " EPOCH 9, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  112.3622, NLL-Loss  111.7933, KL-Loss    0.5699, KL-Weight  0.998\n",
      "VAL Batch 0044/44, Loss  110.1779, NLL-Loss  109.6668, KL-Loss    0.5121, KL-Weight  0.998\n",
      "VAL Epoch 09/10, Mean ELBO   98.7356\n",
      " EPOCH 9, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss   95.5027, NLL-Loss   94.8884, KL-Loss    0.6155, KL-Weight  0.998\n",
      "TEST Batch 0048/48, Loss   93.3039, NLL-Loss   92.7014, KL-Loss    0.6037, KL-Weight  0.998\n",
      "TEST Epoch 09/10, Mean ELBO   95.7505\n",
      "\n",
      "\n",
      " EPOCH 10, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss   82.6543, NLL-Loss   82.0672, KL-Loss    0.5882, KL-Weight  0.998\n",
      "TRAIN Batch 0050/555, Loss   94.2588, NLL-Loss   93.7001, KL-Loss    0.5597, KL-Weight  0.998\n",
      "TRAIN Batch 0100/555, Loss   91.1197, NLL-Loss   90.4778, KL-Loss    0.6429, KL-Weight  0.999\n",
      "TRAIN Batch 0150/555, Loss   81.8236, NLL-Loss   81.1663, KL-Loss    0.6581, KL-Weight  0.999\n",
      "TRAIN Batch 0200/555, Loss   85.5282, NLL-Loss   84.9488, KL-Loss    0.5800, KL-Weight  0.999\n",
      "TRAIN Batch 0250/555, Loss   80.8233, NLL-Loss   80.2528, KL-Loss    0.5711, KL-Weight  0.999\n",
      "TRAIN Batch 0300/555, Loss   86.6000, NLL-Loss   85.9280, KL-Loss    0.6726, KL-Weight  0.999\n",
      "TRAIN Batch 0350/555, Loss   79.8608, NLL-Loss   79.2363, KL-Loss    0.6250, KL-Weight  0.999\n",
      "TRAIN Batch 0400/555, Loss   78.0644, NLL-Loss   77.4083, KL-Loss    0.6566, KL-Weight  0.999\n",
      "TRAIN Batch 0450/555, Loss   92.6650, NLL-Loss   92.1529, KL-Loss    0.5124, KL-Weight  0.999\n",
      "TRAIN Batch 0500/555, Loss   84.4368, NLL-Loss   83.8264, KL-Loss    0.6107, KL-Weight  0.999\n",
      "TRAIN Batch 0550/555, Loss   91.0217, NLL-Loss   90.4379, KL-Loss    0.5841, KL-Weight  1.000\n",
      "TRAIN Batch 0555/555, Loss   82.8196, NLL-Loss   82.3283, KL-Loss    0.4916, KL-Weight  1.000\n",
      "TRAIN Epoch 10/10, Mean ELBO   94.3851\n",
      "Model saved at bin/2019-Feb-01-14:26:59/E10.pytorch\n",
      " EPOCH 10, SPLIT = val\n",
      "-------------------------------------------\n",
      "VAL Batch 0000/44, Loss  112.3561, NLL-Loss  111.7952, KL-Loss    0.5612, KL-Weight  1.000\n",
      "VAL Batch 0044/44, Loss  109.9246, NLL-Loss  109.4347, KL-Loss    0.4902, KL-Weight  1.000\n",
      "VAL Epoch 10/10, Mean ELBO   98.4837\n",
      " EPOCH 10, SPLIT = test\n",
      "-------------------------------------------\n",
      "TEST Batch 0000/48, Loss   95.3907, NLL-Loss   94.7943, KL-Loss    0.5967, KL-Weight  1.000\n",
      "TEST Batch 0048/48, Loss   92.8541, NLL-Loss   92.2903, KL-Loss    0.5640, KL-Weight  1.000\n",
      "TEST Epoch 10/10, Mean ELBO   95.4976\n",
      "-------------------------------------------\n",
      "\n",
      "Training Complete\n",
      "sending model to cuda\n",
      "loading cuda state\n",
      "Model loaded from bin/2019-Feb-01-14:26:59/E9.pytorch\n",
      "wrote samples to 'bin/2019-Feb-01-14:26:59/samples'\n",
      "sending model to cuda\n",
      "SentenceVAE(\n",
      "  (embedding): Embedding(10009, 300)\n",
      "  (embedding_dropout): Dropout(p=0.5)\n",
      "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (hidden2mean): Linear(in_features=256, out_features=20, bias=True)\n",
      "  (hidden2logv): Linear(in_features=256, out_features=20, bias=True)\n",
      "  (latent2hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (outputs2vocab): Linear(in_features=256, out_features=10009, bias=True)\n",
      ")\n",
      "Beginning training at: 2019-Feb-01-14:29:52\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      " EPOCH 1, SPLIT = train\n",
      "-------------------------------------------\n",
      "TRAIN Batch 0000/555, Loss  196.4809, NLL-Loss  196.4799, KL-Loss    0.4895, KL-Weight  0.002\n",
      "TRAIN Batch 0050/555, Loss  118.8169, NLL-Loss  118.7811, KL-Loss   16.4004, KL-Weight  0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-867:\n",
      "Process Process-870:\n",
      "Process Process-865:\n",
      "Process Process-866:\n",
      "Process Process-869:\n",
      "Process Process-868:\n",
      "Process Process-872:\n",
      "Process Process-871:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0100/555, Loss  117.9830, NLL-Loss  117.8951, KL-Loss   35.5392, KL-Weight  0.002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-445530e99818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-684f22dc20e5>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(parameters)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# run the modified experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'False'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-cf94c5cfaa1b>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(args, verbose)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# train the model and record its performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrackers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# args.best_epoch = 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# args.load_checkpoint = 'E3.pytorch'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/floyd/home/models/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, datasets, args, verbose)\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                     \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
