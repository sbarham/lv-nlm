{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ours\n",
    "from corpus.ptb import PTB\n",
    "from corpus.brown import Brown\n",
    "from corpus.gutenberg import Gutenberg\n",
    "from corpus.kjv import Bible\n",
    "from corpus.wikitext_2 import Wikitext2\n",
    "from corpus.wikitext_103 import Wikitext103\n",
    "import util\n",
    "from util.utils import to_var, expierment_name\n",
    "from models.bowman import SentenceVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    # select correct corpus class\n",
    "    assert args.corpus in ['ptb', 'bible', 'gutenberg', 'brown', 'wikitext-2', 'wikitext-103']\n",
    "    if args.corpus == 'ptb':\n",
    "        corpus_class = PTB\n",
    "    elif args.corpus == 'kjv' or args.corpus == 'bible':\n",
    "        nltk.download('gutenberg')\n",
    "        corpus_class = Bible\n",
    "    elif args.corpus == 'gutenberg':\n",
    "        nltk.download('gutenberg')\n",
    "        corpus_class = Gutenberg\n",
    "    elif args.corpus == 'brown':\n",
    "        nltk.download('brown')\n",
    "        corpus_class = Brown\n",
    "    elif args.corpus == 'wikitext-2':\n",
    "        corpus_class = Wikitext2\n",
    "    elif args.corpus == 'wikitext-103':\n",
    "        corpus_class = Wikitext103\n",
    "    \n",
    "    # prepare for splits\n",
    "    splits = [util.TRAIN, util.VAL] + ([util.TEST] if args.test else [])\n",
    "    datasets = OrderedDict()\n",
    "    datasets.splits = splits\n",
    "    \n",
    "    # create train, validation, and possibly test split\n",
    "    for split in datasets.splits:\n",
    "        datasets[split] = corpus_class(\n",
    "            data_dir=args.data_dir,\n",
    "            split=split,\n",
    "            create_data=args.create_data,\n",
    "            max_sequence_length=args.max_sequence_length,\n",
    "            min_occ=args.min_occ,\n",
    "            embeddings=args.embeddings\n",
    "        )\n",
    "    \n",
    "    # return the splits\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(args, datasets):\n",
    "    model = SentenceVAE(\n",
    "            vocab_size=datasets[util.TRAIN].vocab_size,\n",
    "            sos_idx=datasets[util.TRAIN].sos_idx,\n",
    "            eos_idx=datasets[util.TRAIN].eos_idx,\n",
    "            pad_idx=datasets[util.TRAIN].pad_idx,\n",
    "            unk_idx=datasets[util.TRAIN].unk_idx,\n",
    "            max_sequence_length=args.max_sequence_length,\n",
    "            embedding_size=args.embedding_size,\n",
    "            rnn_type=args.rnn_type,\n",
    "            hidden_size=args.hidden_size,\n",
    "            word_dropout=args.word_dropout,\n",
    "            embedding_dropout=args.embedding_dropout,\n",
    "            latent_size=args.latent_size,\n",
    "            num_layers=args.num_layers,\n",
    "            bidirectional=args.bidirectional\n",
    "        )\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_anneal_function(anneal_function, step, k, x0):\n",
    "    if anneal_function == 'logistic':\n",
    "        return float(1.0 / (1.0 + np.exp(-k * (step - x0))))\n",
    "    elif anneal_function == 'linear':\n",
    "        return min(1.0, step / x0)\n",
    "    elif anneal_function == 'const':\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(logp, target, length, mean, logv, anneal_function, step, k, x0, pad_idx):\n",
    "    NLL = torch.nn.NLLLoss(reduction='sum', ignore_index=pad_idx)\n",
    "    \n",
    "    # cut-off unnecessary padding from target, and flatten\n",
    "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(2))\n",
    "        \n",
    "    # Negative Log Likelihood\n",
    "    NLL_loss = NLL(logp, target)\n",
    "\n",
    "    # KL Divergence\n",
    "    KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n",
    "    KL_weight = kl_anneal_function(anneal_function, step, k, x0)\n",
    "\n",
    "    return NLL_loss, KL_loss, KL_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx2word(sents, i2w, pad_idx):\n",
    "    sent_str = [str()] * len(sents)\n",
    "\n",
    "    for sent_idx, sent in enumerate(sents):\n",
    "        for word_id in sent:\n",
    "            try:\n",
    "                word_id = word_id.item()\n",
    "            except: pass\n",
    "            \n",
    "            if word_id == pad_idx:\n",
    "                break\n",
    "            \n",
    "            sent_str[sent_idx] += (i2w[word_id] + \" \")\n",
    "\n",
    "        sent_str[sent_idx] = sent_str[sent_idx].strip()\n",
    "\n",
    "\n",
    "    return sent_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model/Runtime Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Dataset/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, datasets, args):\n",
    "    print(model)\n",
    "    \n",
    "    timestamp = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
    "\n",
    "    # create the directory for saving this model\n",
    "    args.save_model_path = os.path.join(util.MODEL_DIR, timestamp)\n",
    "    os.makedirs(args.save_model_path)\n",
    "    \n",
    "    # create the optimizer, the tracker, and initialize the step to 0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    trackers = {split: defaultdict(list) for split in datasets.splits}\n",
    "    step = 0\n",
    "    best = float('inf')\n",
    "    \n",
    "    # get the pad index, for convenience\n",
    "    pad_idx = datasets['train'].get_w2i()['<pad>']\n",
    "    \n",
    "    # go!\n",
    "    for epoch in range(args.epochs):\n",
    "        for split in datasets.splits:\n",
    "            print(\"SPLIT = {}\".format(split))\n",
    "            \n",
    "            data_loader = DataLoader(\n",
    "                dataset=datasets[split],\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=split=='train',\n",
    "                num_workers=cpu_count(),\n",
    "                pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            # Enable/Disable Dropout\n",
    "            if split == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for iteration, batch in enumerate(data_loader):\n",
    "                batch_size = batch['input'].size(0)\n",
    "\n",
    "                for k, v in batch.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        batch[k] = to_var(v)\n",
    "\n",
    "                # Forward pass\n",
    "                logp, mean, logv, z = model(batch['input'], batch['length'])\n",
    "\n",
    "                # loss calculation\n",
    "                NLL_loss, KL_loss, KL_weight = loss_fn(logp, batch['target'],\n",
    "                    batch['length'], mean, logv, args.anneal_function, step, args.k, args.x0, pad_idx)\n",
    "\n",
    "                loss = (NLL_loss + KL_weight * KL_loss) / batch_size\n",
    "\n",
    "                # backward + optimization\n",
    "                if split == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    step += 1\n",
    "\n",
    "                if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                    print(\"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"\n",
    "                        % (split.upper(),\n",
    "                           iteration,\n",
    "                           len(data_loader) - 1,\n",
    "                           loss.item(),\n",
    "                           NLL_loss.item() / batch_size,\n",
    "                           KL_loss.item() / batch_size,\n",
    "                           KL_weight))\n",
    "\n",
    "                trackers[split]['ELBO'].append(loss.item())\n",
    "                trackers[split]['NLL'].append(NLL_loss.item() / batch_size)\n",
    "                trackers[split]['KLL'].append(KL_loss.item() / batch_size)\n",
    "                trackers[split]['KL_weight'].append(KL_weight)\n",
    "                \n",
    "#                 if split == 'valid':\n",
    "#                     i2w = datasets['train'].get_i2w()\n",
    "#                     trackers[split]['target_sents'] += idx2word(batch['target'].data, i2w=i2w, pad_idx=pad_idx)\n",
    "#                     trackers[split]['z'].append(z.tolist())\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            END OF BATCH\n",
    "            \"\"\"\n",
    "            print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\" % (split.upper(), epoch, args.epochs, np.mean(trackers[split]['ELBO'])))\n",
    "\n",
    "            # save a dump of all sentences and the encoded latent space\n",
    "#             if split == 'valid':\n",
    "#                 dump = {'target_sents':trackers[split]['target_sents'], 'z':trackers[split]['z']}\n",
    "#                 if not os.path.exists(os.path.join('dumps', ts)):\n",
    "#                     os.makedirs('dumps/' + ts)\n",
    "#                 with open(os.path.join('dumps/'+ts+'/valid_E%i.pickle' % epoch), 'wb') as dump_file:\n",
    "#                     pickle.dump(dump, dump_file)\n",
    "\n",
    "            # save checkpoint\n",
    "            if split == 'train':                \n",
    "                # save checkpoint\n",
    "                checkpoint_path = os.path.join(args.save_model_path, \"E%i.pytorch\" % (epoch))\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(\"Model saved at %s\" % checkpoint_path)\n",
    "                \n",
    "                # check if best checkpoint so far\n",
    "                if np.mean(trackers[split]['ELBO']) < best:\n",
    "                    best = np.mean(trackers[split]['ELBO'])\n",
    "                    args.load_checkpoint = 'E{}.pytorch'.format(epoch)\n",
    "                \n",
    "    return trackers, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exponential_smoothing(ys, beta=0.8, ub=math.inf, lb=-math.inf):\n",
    "    \"\"\"\n",
    "    This is ugly, and I should have used a comprehension, but\n",
    "    it'll get the job done. I made it a function because I suspect\n",
    "    I may need it later.\n",
    "    \"\"\"\n",
    "    smooth_ys = [ys[0]]\n",
    "    for y in ys:\n",
    "        if y > ub or y < lb:\n",
    "            smooth_ys.append(smooth_ys[-1])\n",
    "        else:\n",
    "            smooth_ys.append(beta * smooth_ys[-1] + (1 - beta) * y)\n",
    "    return smooth_ys[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(ELBO, NLL, KL, title, fname=None, xlabel=\"Epochs\", ylabel=\"Measurements\", hline=None, epochs=None):\n",
    "    \"\"\"\n",
    "    Just a *slight* abstraction over pyplot to ease development a bit.\n",
    "    \"\"\"\n",
    "    xs = list(range(len(ELBO)))\n",
    "    if epochs is not None:\n",
    "        xs = [x / len(xs) * epochs for x in xs]\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    if hline:\n",
    "        plt.axhline(y=hline, color='r', linestyle='-')\n",
    "    \n",
    "    plt.plot(xs, ELBO, label=\"ELBO\")\n",
    "    plt.plot(xs, NLL, label=\"NLL Loss\", c='blue')\n",
    "    plt.plot(xs, KL, label=\"KL Loss\", c='red')\n",
    "    plt.legend()\n",
    "    \n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_elbo(ELBO, fname=None, title='ELBO', xlabel=\"Epochs\", ylabel=\"ELBO\", hline=None, epochs=None):\n",
    "    \"\"\"\n",
    "    Just a *slight* abstraction over pyplot to ease development a bit.\n",
    "    \"\"\"\n",
    "    xs = list(range(len(ELBO)))\n",
    "    if epochs is not None:\n",
    "        xs = [x / len(xs) * epochs for x in xs]\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    plt.plot(xs, ELBO, label=\"ELBO\")\n",
    "    plt.legend()\n",
    "        \n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(trackers, datasets, args):\n",
    "    for split in datasets.splits:\n",
    "        fname = '{}_perf:emb{}-z{}-lstm{}-maxlen{}'.format(\n",
    "            split,\n",
    "            args.embedding_size,\n",
    "            args.latent_size,\n",
    "            args.hidden_size,\n",
    "            args.max_sequence_length\n",
    "        )\n",
    "        \n",
    "        fname = os.path.join(args.save_model_path, fname)\n",
    "        \n",
    "        plot(\n",
    "            fname=fname,\n",
    "            ELBO=exponential_smoothing(trackers[split]['ELBO']),\n",
    "            KL=exponential_smoothing(trackers[split]['KLL']),\n",
    "            NLL=exponential_smoothing(trackers[split]['NLL']),\n",
    "            title='S-VAE *{}* Performance\\n(Mikolov\\'s Simplified PTB, max length={})'.format(\n",
    "                split,\n",
    "                args.max_sequence_length\n",
    "            ),\n",
    "            epochs=args.epochs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolate(start, end, steps):\n",
    "    steps = steps + 2\n",
    "    \n",
    "    interpolation = np.zeros((start.shape[0], steps))\n",
    "\n",
    "    for dim, (s, e) in enumerate(zip(start, end)):\n",
    "        interpolation[dim] = np.linspace(s, e, steps)\n",
    "\n",
    "    return interpolation.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_args(args):\n",
    "    fname = os.path.join(args.save_model_path, 'args')\n",
    "    with open(fname, 'w+') as file:\n",
    "        lines = ['{}: {}\\n'.format(key, val) for key, val in vars(args).items()]\n",
    "        file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_trackers(trackers, args):\n",
    "    fname = os.path.join(args.save_model_path, 'trackers.pickle')\n",
    "    with open(fname, 'wb') as file:\n",
    "        pickle.dump(trackers, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(args):\n",
    "    with open(args.data_dir + '/ptb.vocab.pickle', 'rb') as file:\n",
    "        vocab = pickle.load(file)\n",
    "\n",
    "    w2i, i2w = vocab['w2i'], vocab['i2w']\n",
    "\n",
    "    model = SentenceVAE(\n",
    "        vocab_size=len(w2i),\n",
    "        sos_idx=w2i['<sos>'],\n",
    "        eos_idx=w2i['<eos>'],\n",
    "        pad_idx=w2i['<pad>'],\n",
    "        unk_idx=w2i['<unk>'],\n",
    "        max_sequence_length=args.max_sequence_length,\n",
    "        embedding_size=args.embedding_size,\n",
    "        rnn_type=args.rnn_type,\n",
    "        hidden_size=args.hidden_size,\n",
    "        word_dropout=args.word_dropout,\n",
    "        embedding_dropout=args.embedding_dropout,\n",
    "        latent_size=args.latent_size,\n",
    "        num_layers=args.num_layers,\n",
    "        bidirectional=args.bidirectional\n",
    "        )\n",
    "\n",
    "    checkpoint_path = os.path.join(args.save_model_path, args.load_checkpoint)\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(checkpoint_path)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "    print(\"Model loaded from %s\" % (checkpoint_path))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    fname = os.path.join(args.save_model_path, 'samples')\n",
    "    lines = []\n",
    "    with open(fname, 'w+') as file:\n",
    "        samples, z = model.inference(n=args.num_samples)\n",
    "        lines += ['----------SAMPLES----------']\n",
    "        lines += [line + '\\n' for line in idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])]\n",
    "        lines += ['\\n']\n",
    "\n",
    "        z1 = torch.randn([args.latent_size]).numpy()\n",
    "        z2 = torch.randn([args.latent_size]).numpy()\n",
    "        z = to_var(torch.from_numpy(interpolate(start=z1, end=z2, steps=8)).float())\n",
    "        samples, _ = model.inference(z=z)\n",
    "        lines += ['-------SELF-GENERATED INTERPOLATION-------']\n",
    "        lines += [line + '\\n' for line in idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])]\n",
    "        lines += ['\\n']\n",
    "\n",
    "        # pick two random sentences\n",
    "        i = random.randint(0, len(datasets['train']))\n",
    "        j = random.randint(0, len(datasets['train']))\n",
    "\n",
    "        s_i = torch.tensor([datasets['train'][i]['input']])\n",
    "        s_j = torch.tensor([datasets['train'][j]['input']])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, _, _, z_i = model(s_i, torch.tensor([1]))\n",
    "            _, _, _, z_j = model(s_j, torch.tensor([1]))\n",
    "            \n",
    "        z1, z2 = z_i.squeeze().numpy(), z_j.squeeze().numpy()\n",
    "        z = to_var(torch.from_numpy(interpolate(start=z1, end=z2, steps=8)).float())\n",
    "        samples, _ = model.inference(z=z)\n",
    "        lines += ['-------DATA-DRIVEN INTERPOLATION----------']\n",
    "        lines += [line + '\\n'  for line in idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])]\n",
    "        lines += ['\\n']\n",
    "        \n",
    "        print(\"wrote samples to '{}'\".format(fname))\n",
    "        file.writelines(lines)\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sentences(datasets):\n",
    "    data = datasets['train'] + datasets['valid'] + datasets['test']\n",
    "    \n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(datasets):\n",
    "    data = datasets['train'] + datasets['valid'] + datasets['test']\n",
    "    total = 0\n",
    "    for sent in data:\n",
    "        total = total + sent['length']\n",
    "        \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set all model/runtime arguments\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.data_dir = 'data'\n",
    "args.create_data = True\n",
    "args.max_sequence_length = 50\n",
    "args.min_occ = 1\n",
    "args.test = True\n",
    "args.epochs = 10\n",
    "args.batch_size = 64\n",
    "args.learning_rate = 0.001\n",
    "\n",
    "args.corpus = 'bible'\n",
    "\n",
    "args.num_samples = 10\n",
    "\n",
    "args.embeddings = True\n",
    "args.embedding_size = 300\n",
    "args.rnn_type = 'gru'\n",
    "args.hidden_size = 512\n",
    "args.num_layers = 1\n",
    "args.bidirectional = False\n",
    "args.latent_size = 32\n",
    "args.word_dropout = 0.0\n",
    "args.embedding_dropout = 0.5\n",
    "\n",
    "args.anneal_function = 'logistic'\n",
    "args.k = 0.0025\n",
    "args.x0 = 2500\n",
    "\n",
    "args.print_every = 50\n",
    "args.tensorboard_logging = False\n",
    "args.logdir = 'logs'\n",
    "args.save_model_path = 'bin/good25'\n",
    "args.load_checkpoint = 'E9.pytorch'\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear', 'const']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "assert args.corpus in ['ptb', 'bible', 'gutenberg', 'brown', 'wikitext-2', 'wikitext-103']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(args):\n",
    "    # create the datasets and model\n",
    "    datasets = create_datasets(args)\n",
    "\n",
    "    # create a new model\n",
    "    model = create_model(args, datasets)\n",
    "    \n",
    "    # train the model and record its performance\n",
    "    trackers, model = train(model, datasets, args)\n",
    "    \n",
    "    # write args to file\n",
    "    save_args(args)\n",
    "    \n",
    "    # save the trackers\n",
    "    save_trackers(trackers, args)\n",
    "    \n",
    "    # graph the results and save\n",
    "    graph(trackers, datasets, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "Preprocessing KJV Bible *train* data:\n",
      "------------------------------------------\n",
      "Creating vocab file ...\n",
      "\t[Getting word counts]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca0231e66bc48eb994f383d2782f7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30103), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Creating dictionaries]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f5008d4cfa4d2cbf11cf6f2b98777a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13769), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Loading pretrained GLOVE embeddings -- this may take a while the first time]\n",
      "Loaded 400000 words\n",
      "\t[Mapping vocab to GLOVE embeddings]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78d932893a2417bafee225867aa1ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13773), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created (13773 word types)!\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063d7488ea7e4cd1be7de41b8fd4e88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, with:\n",
      "\t20370 sentences\n",
      "\t554202 word tokens\n",
      "\t27.206774668630338 avg. sentence length\n",
      "\n",
      "\n",
      "Preprocessing KJV Bible *val* data:\n",
      "------------------------------------------\n",
      "Loading vocab file ...\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3dada32b584759a7f08ca2af4a44eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, with:\n",
      "\t2541 sentences\n",
      "\t69072 word tokens\n",
      "\t27.182998819362457 avg. sentence length\n",
      "\n",
      "\n",
      "Preprocessing KJV Bible *test* data:\n",
      "------------------------------------------\n",
      "Loading vocab file ...\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cb70c3d1ec402da86aa6ab59df8c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, with:\n",
      "\t2572 sentences\n",
      "\t69381 word tokens\n",
      "\t26.975505443234837 avg. sentence length\n",
      "\n",
      "\n",
      "SentenceVAE(\n",
      "  (embedding): Embedding(13773, 300)\n",
      "  (embedding_dropout): Dropout(p=0.5)\n",
      "  (encoder_rnn): GRU(300, 512, batch_first=True)\n",
      "  (decoder_rnn): GRU(300, 512, batch_first=True)\n",
      "  (hidden2mean): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (hidden2logv): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (latent2hidden): Linear(in_features=32, out_features=512, bias=True)\n",
      "  (outputs2vocab): Linear(in_features=512, out_features=13773, bias=True)\n",
      ")\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss  276.3725, NLL-Loss  276.3715, KL-Loss    0.5062, KL-Weight  0.002\n",
      "TRAIN Batch 0050/318, Loss  145.1693, NLL-Loss  145.0179, KL-Loss   69.3865, KL-Weight  0.002\n",
      "TRAIN Batch 0100/318, Loss  130.3873, NLL-Loss  130.1690, KL-Loss   88.3018, KL-Weight  0.002\n",
      "TRAIN Batch 0150/318, Loss  113.3534, NLL-Loss  113.1229, KL-Loss   82.3050, KL-Weight  0.003\n",
      "TRAIN Batch 0200/318, Loss  104.9447, NLL-Loss  104.6173, KL-Loss  103.1919, KL-Weight  0.003\n",
      "TRAIN Batch 0250/318, Loss  108.6600, NLL-Loss  108.2500, KL-Loss  114.1047, KL-Weight  0.004\n",
      "TRAIN Batch 0300/318, Loss   97.5914, NLL-Loss   97.0900, KL-Loss  123.2049, KL-Weight  0.004\n",
      "TRAIN Batch 0318/318, Loss   88.8607, NLL-Loss   88.3151, KL-Loss  128.1932, KL-Weight  0.004\n",
      "TRAIN Epoch 00/10, Mean ELBO  123.2442\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E0.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss  106.8100, NLL-Loss  106.2552, KL-Loss  130.0131, KL-Weight  0.004\n",
      "VAL Batch 0039/39, Loss  103.9543, NLL-Loss  103.4377, KL-Loss  121.0525, KL-Weight  0.004\n",
      "VAL Epoch 00/10, Mean ELBO  100.2000\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   92.3527, NLL-Loss   91.8028, KL-Loss  128.8661, KL-Weight  0.004\n",
      "TEST Batch 0040/40, Loss  124.7171, NLL-Loss  124.2063, KL-Loss  119.7032, KL-Weight  0.004\n",
      "TEST Epoch 00/10, Mean ELBO   99.5250\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss  103.7226, NLL-Loss  103.2238, KL-Loss  116.8842, KL-Weight  0.004\n",
      "TRAIN Batch 0050/318, Loss   96.9690, NLL-Loss   96.3508, KL-Loss  127.9301, KL-Weight  0.005\n",
      "TRAIN Batch 0100/318, Loss   84.9614, NLL-Loss   84.2872, KL-Loss  123.1988, KL-Weight  0.005\n",
      "TRAIN Batch 0150/318, Loss   84.5386, NLL-Loss   83.7431, KL-Loss  128.3797, KL-Weight  0.006\n",
      "TRAIN Batch 0200/318, Loss   92.4988, NLL-Loss   91.5973, KL-Loss  128.4834, KL-Weight  0.007\n",
      "TRAIN Batch 0250/318, Loss   89.7416, NLL-Loss   88.7396, KL-Loss  126.1536, KL-Weight  0.008\n",
      "TRAIN Batch 0300/318, Loss  103.8530, NLL-Loss  102.7419, KL-Loss  123.5730, KL-Weight  0.009\n",
      "TRAIN Batch 0318/318, Loss   69.5662, NLL-Loss   68.4693, KL-Loss  116.6704, KL-Weight  0.009\n",
      "TRAIN Epoch 01/10, Mean ELBO  107.6583\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E1.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss   94.8707, NLL-Loss   93.6787, KL-Loss  126.4803, KL-Weight  0.009\n",
      "VAL Batch 0039/39, Loss   90.6310, NLL-Loss   89.4730, KL-Loss  122.8808, KL-Weight  0.009\n",
      "VAL Epoch 01/10, Mean ELBO   94.1906\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   81.3765, NLL-Loss   80.1851, KL-Loss  126.4163, KL-Weight  0.009\n",
      "TEST Batch 0040/40, Loss  111.8247, NLL-Loss  110.6882, KL-Loss  120.5930, KL-Weight  0.009\n",
      "TEST Epoch 01/10, Mean ELBO   93.4568\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss   78.5512, NLL-Loss   77.3851, KL-Loss  123.7365, KL-Weight  0.009\n",
      "TRAIN Batch 0050/318, Loss   81.3479, NLL-Loss   79.9439, KL-Loss  131.6378, KL-Weight  0.011\n",
      "TRAIN Batch 0100/318, Loss   77.9412, NLL-Loss   76.3608, KL-Loss  130.9534, KL-Weight  0.012\n",
      "TRAIN Batch 0150/318, Loss   75.0036, NLL-Loss   73.3390, KL-Loss  121.9158, KL-Weight  0.014\n",
      "TRAIN Batch 0200/318, Loss   76.4273, NLL-Loss   74.5383, KL-Loss  122.3142, KL-Weight  0.015\n",
      "TRAIN Batch 0250/318, Loss   89.8586, NLL-Loss   87.8216, KL-Loss  116.6399, KL-Weight  0.017\n",
      "TRAIN Batch 0300/318, Loss   78.1092, NLL-Loss   75.8311, KL-Loss  115.3856, KL-Weight  0.020\n",
      "TRAIN Batch 0318/318, Loss   66.4257, NLL-Loss   64.1635, KL-Loss  109.6371, KL-Weight  0.021\n",
      "TRAIN Epoch 02/10, Mean ELBO   98.3612\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E2.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss   88.9111, NLL-Loss   86.5597, KL-Loss  113.6814, KL-Weight  0.021\n",
      "VAL Batch 0039/39, Loss   86.1509, NLL-Loss   83.8135, KL-Loss  113.0044, KL-Weight  0.021\n",
      "VAL Epoch 02/10, Mean ELBO   90.4662\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   76.1500, NLL-Loss   73.8171, KL-Loss  112.7888, KL-Weight  0.021\n",
      "TEST Batch 0040/40, Loss  107.2101, NLL-Loss  104.9168, KL-Loss  110.8754, KL-Weight  0.021\n",
      "TEST Epoch 02/10, Mean ELBO   89.7513\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss   78.0865, NLL-Loss   75.7643, KL-Loss  112.2709, KL-Weight  0.021\n",
      "TRAIN Batch 0050/318, Loss   73.6216, NLL-Loss   70.9957, KL-Loss  112.3460, KL-Weight  0.023\n",
      "TRAIN Batch 0100/318, Loss   70.6093, NLL-Loss   67.6889, KL-Loss  110.6067, KL-Weight  0.026\n",
      "TRAIN Batch 0150/318, Loss   74.7825, NLL-Loss   71.6160, KL-Loss  106.2054, KL-Weight  0.030\n",
      "TRAIN Batch 0200/318, Loss   62.5035, NLL-Loss   59.1576, KL-Loss   99.4328, KL-Weight  0.034\n",
      "TRAIN Batch 0250/318, Loss   75.8274, NLL-Loss   72.0404, KL-Loss   99.7610, KL-Weight  0.038\n",
      "TRAIN Batch 0300/318, Loss   70.2119, NLL-Loss   66.2507, KL-Loss   92.5542, KL-Weight  0.043\n",
      "TRAIN Batch 0318/318, Loss   84.7439, NLL-Loss   80.6941, KL-Loss   90.6395, KL-Weight  0.045\n",
      "TRAIN Epoch 03/10, Mean ELBO   91.9237\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E3.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss   88.0157, NLL-Loss   83.9189, KL-Loss   91.4723, KL-Weight  0.045\n",
      "VAL Batch 0039/39, Loss   83.9284, NLL-Loss   79.8791, KL-Loss   90.4099, KL-Weight  0.045\n",
      "VAL Epoch 03/10, Mean ELBO   88.1873\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   75.4238, NLL-Loss   71.3956, KL-Loss   89.9407, KL-Weight  0.045\n",
      "TEST Batch 0040/40, Loss  103.9692, NLL-Loss   99.9110, KL-Loss   90.6089, KL-Weight  0.045\n",
      "TEST Epoch 03/10, Mean ELBO   87.4783\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss   69.4845, NLL-Loss   65.4972, KL-Loss   89.0273, KL-Weight  0.045\n",
      "TRAIN Batch 0050/318, Loss   66.1690, NLL-Loss   61.5714, KL-Loss   91.1316, KL-Weight  0.050\n",
      "TRAIN Batch 0100/318, Loss   65.8956, NLL-Loss   61.1286, KL-Loss   83.9473, KL-Weight  0.057\n",
      "TRAIN Batch 0150/318, Loss   76.4207, NLL-Loss   71.0231, KL-Loss   84.5163, KL-Weight  0.064\n",
      "TRAIN Batch 0200/318, Loss   67.0730, NLL-Loss   61.3377, KL-Loss   79.9255, KL-Weight  0.072\n",
      "TRAIN Batch 0250/318, Loss   75.7490, NLL-Loss   69.8260, KL-Loss   73.5398, KL-Weight  0.081\n",
      "TRAIN Batch 0300/318, Loss   70.4425, NLL-Loss   64.1140, KL-Loss   70.0842, KL-Weight  0.090\n",
      "TRAIN Batch 0318/318, Loss   63.3542, NLL-Loss   57.1929, KL-Loss   65.5015, KL-Weight  0.094\n",
      "TRAIN Epoch 04/10, Mean ELBO   87.2841\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E4.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss   89.0390, NLL-Loss   82.4739, KL-Loss   69.6365, KL-Weight  0.094\n",
      "VAL Batch 0039/39, Loss   84.6058, NLL-Loss   78.2050, KL-Loss   67.8933, KL-Weight  0.094\n",
      "VAL Epoch 04/10, Mean ELBO   86.9874\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   76.2360, NLL-Loss   69.9118, KL-Loss   67.0808, KL-Weight  0.094\n",
      "TEST Batch 0040/40, Loss  106.6753, NLL-Loss  100.2040, KL-Loss   68.6413, KL-Weight  0.094\n",
      "TEST Epoch 04/10, Mean ELBO   86.3063\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss   64.6623, NLL-Loss   58.2374, KL-Loss   68.1494, KL-Weight  0.094\n",
      "TRAIN Batch 0050/318, Loss   65.5725, NLL-Loss   58.3567, KL-Loss   68.3928, KL-Weight  0.106\n",
      "TRAIN Batch 0100/318, Loss   66.8704, NLL-Loss   58.8657, KL-Loss   67.8963, KL-Weight  0.118\n",
      "TRAIN Batch 0150/318, Loss   62.5933, NLL-Loss   54.3868, KL-Loss   62.3929, KL-Weight  0.132\n",
      "TRAIN Batch 0200/318, Loss   68.2251, NLL-Loss   59.8305, KL-Loss   57.3103, KL-Weight  0.146\n",
      "TRAIN Batch 0250/318, Loss   66.8910, NLL-Loss   57.7356, KL-Loss   56.2352, KL-Weight  0.163\n",
      "TRAIN Batch 0300/318, Loss   73.3229, NLL-Loss   63.7142, KL-Loss   53.2136, KL-Weight  0.181\n",
      "TRAIN Batch 0318/318, Loss   86.2572, NLL-Loss   75.9884, KL-Loss   54.8191, KL-Weight  0.187\n",
      "TRAIN Epoch 05/10, Mean ELBO   84.0756\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E5.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss   92.5565, NLL-Loss   82.7587, KL-Loss   52.1981, KL-Weight  0.188\n",
      "VAL Batch 0039/39, Loss   87.5900, NLL-Loss   78.0148, KL-Loss   51.0122, KL-Weight  0.188\n",
      "VAL Epoch 05/10, Mean ELBO   86.7298\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   79.5610, NLL-Loss   70.2458, KL-Loss   49.6268, KL-Weight  0.188\n",
      "TEST Batch 0040/40, Loss  107.7319, NLL-Loss   97.8354, KL-Loss   52.7238, KL-Weight  0.188\n",
      "TEST Epoch 05/10, Mean ELBO   86.0280\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss   62.4965, NLL-Loss   53.1241, KL-Loss   49.9322, KL-Weight  0.188\n",
      "TRAIN Batch 0050/318, Loss   67.8281, NLL-Loss   57.3422, KL-Loss   50.5319, KL-Weight  0.208\n",
      "TRAIN Batch 0100/318, Loss   63.2810, NLL-Loss   52.6298, KL-Loss   46.5490, KL-Weight  0.229\n",
      "TRAIN Batch 0150/318, Loss   72.8742, NLL-Loss   61.0211, KL-Loss   47.1074, KL-Weight  0.252\n",
      "TRAIN Batch 0200/318, Loss   81.6241, NLL-Loss   69.4228, KL-Loss   44.2270, KL-Weight  0.276\n",
      "TRAIN Batch 0250/318, Loss   68.6212, NLL-Loss   56.8463, KL-Loss   39.0498, KL-Weight  0.302\n",
      "TRAIN Batch 0300/318, Loss   78.3888, NLL-Loss   65.4925, KL-Loss   39.2587, KL-Weight  0.328\n",
      "TRAIN Batch 0318/318, Loss   79.6311, NLL-Loss   65.6301, KL-Loss   41.3620, KL-Weight  0.338\n",
      "TRAIN Epoch 06/10, Mean ELBO   82.0793\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E6.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss   97.6725, NLL-Loss   84.5060, KL-Loss   38.8325, KL-Weight  0.339\n",
      "VAL Batch 0039/39, Loss   93.3797, NLL-Loss   80.4798, KL-Loss   38.0466, KL-Weight  0.339\n",
      "VAL Epoch 06/10, Mean ELBO   87.3059\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   85.6463, NLL-Loss   73.1517, KL-Loss   36.8509, KL-Weight  0.339\n",
      "TEST Batch 0040/40, Loss  115.3324, NLL-Loss  102.0444, KL-Loss   39.1911, KL-Weight  0.339\n",
      "TEST Epoch 06/10, Mean ELBO   86.6155\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss   73.1743, NLL-Loss   60.4333, KL-Loss   37.5777, KL-Weight  0.339\n",
      "TRAIN Batch 0050/318, Loss   67.2437, NLL-Loss   53.2564, KL-Loss   38.0498, KL-Weight  0.368\n",
      "TRAIN Batch 0100/318, Loss   71.2318, NLL-Loss   57.0225, KL-Loss   35.7812, KL-Weight  0.397\n",
      "TRAIN Batch 0150/318, Loss   72.2752, NLL-Loss   57.3585, KL-Loss   34.9017, KL-Weight  0.427\n",
      "TRAIN Batch 0200/318, Loss   77.9915, NLL-Loss   62.3685, KL-Loss   34.0949, KL-Weight  0.458\n",
      "TRAIN Batch 0250/318, Loss   76.2402, NLL-Loss   60.7687, KL-Loss   31.6146, KL-Weight  0.489\n",
      "TRAIN Batch 0300/318, Loss   76.6767, NLL-Loss   60.6748, KL-Loss   30.7366, KL-Weight  0.521\n",
      "TRAIN Batch 0318/318, Loss   75.5482, NLL-Loss   60.2915, KL-Loss   28.6870, KL-Weight  0.532\n",
      "TRAIN Epoch 07/10, Mean ELBO   81.0567\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E7.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss  103.5392, NLL-Loss   87.5348, KL-Loss   30.0578, KL-Weight  0.532\n",
      "VAL Batch 0039/39, Loss   98.7158, NLL-Loss   82.9140, KL-Loss   29.6773, KL-Weight  0.532\n",
      "VAL Epoch 07/10, Mean ELBO   88.4254\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   90.0032, NLL-Loss   74.8918, KL-Loss   28.3805, KL-Weight  0.532\n",
      "TEST Batch 0040/40, Loss  120.9367, NLL-Loss  104.3158, KL-Loss   31.2155, KL-Weight  0.532\n",
      "TEST Epoch 07/10, Mean ELBO   87.7271\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss   79.6521, NLL-Loss   63.1363, KL-Loss   31.0182, KL-Weight  0.532\n",
      "TRAIN Batch 0050/318, Loss   74.3435, NLL-Loss   57.8233, KL-Loss   29.3221, KL-Weight  0.563\n",
      "TRAIN Batch 0100/318, Loss   74.3772, NLL-Loss   57.8039, KL-Loss   27.9072, KL-Weight  0.594\n",
      "TRAIN Batch 0150/318, Loss   79.9315, NLL-Loss   62.9650, KL-Loss   27.2059, KL-Weight  0.624\n",
      "TRAIN Batch 0200/318, Loss   78.8634, NLL-Loss   61.8528, KL-Loss   26.0703, KL-Weight  0.652\n",
      "TRAIN Batch 0250/318, Loss   79.6588, NLL-Loss   62.4634, KL-Loss   25.2775, KL-Weight  0.680\n",
      "TRAIN Batch 0300/318, Loss   77.2476, NLL-Loss   60.2353, KL-Loss   24.0688, KL-Weight  0.707\n",
      "TRAIN Batch 0318/318, Loss   84.4208, NLL-Loss   65.8046, KL-Loss   25.9980, KL-Weight  0.716\n",
      "TRAIN Epoch 08/10, Mean ELBO   80.6428\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E8.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss  106.6699, NLL-Loss   89.6334, KL-Loss   23.7751, KL-Weight  0.717\n",
      "VAL Batch 0039/39, Loss  103.0248, NLL-Loss   86.4124, KL-Loss   23.1832, KL-Weight  0.717\n",
      "VAL Epoch 08/10, Mean ELBO   89.6525\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   93.5449, NLL-Loss   77.4554, KL-Loss   22.4535, KL-Weight  0.717\n",
      "TEST Batch 0040/40, Loss  123.5413, NLL-Loss  106.2166, KL-Loss   24.1774, KL-Weight  0.717\n",
      "TEST Epoch 08/10, Mean ELBO   88.9695\n",
      "SPLIT = train\n",
      "TRAIN Batch 0000/318, Loss   79.4147, NLL-Loss   62.4396, KL-Loss   23.6894, KL-Weight  0.717\n",
      "TRAIN Batch 0050/318, Loss   76.0828, NLL-Loss   59.0250, KL-Loss   23.0121, KL-Weight  0.741\n",
      "TRAIN Batch 0100/318, Loss   83.5854, NLL-Loss   65.0521, KL-Loss   24.2425, KL-Weight  0.764\n",
      "TRAIN Batch 0150/318, Loss   75.8947, NLL-Loss   58.2419, KL-Loss   22.4517, KL-Weight  0.786\n",
      "TRAIN Batch 0200/318, Loss   82.9314, NLL-Loss   66.0725, KL-Loss   20.9035, KL-Weight  0.807\n",
      "TRAIN Batch 0250/318, Loss   76.9781, NLL-Loss   59.7310, KL-Loss   20.8987, KL-Weight  0.825\n",
      "TRAIN Batch 0300/318, Loss   89.5916, NLL-Loss   71.2709, KL-Loss   21.7437, KL-Weight  0.843\n",
      "TRAIN Batch 0318/318, Loss   76.7849, NLL-Loss   59.3604, KL-Loss   20.5368, KL-Weight  0.848\n",
      "TRAIN Epoch 09/10, Mean ELBO   80.4993\n",
      "Model saved at bin/2019-Jan-18-03:03:09/E9.pytorch\n",
      "SPLIT = val\n",
      "VAL Batch 0000/39, Loss  109.9072, NLL-Loss   92.3752, KL-Loss   20.6557, KL-Weight  0.849\n",
      "VAL Batch 0039/39, Loss  104.5244, NLL-Loss   87.7449, KL-Loss   19.7691, KL-Weight  0.849\n",
      "VAL Epoch 09/10, Mean ELBO   90.8990\n",
      "SPLIT = test\n",
      "TEST Batch 0000/40, Loss   96.4441, NLL-Loss   79.9409, KL-Loss   19.4436, KL-Weight  0.849\n",
      "TEST Batch 0040/40, Loss  125.7754, NLL-Loss  107.9313, KL-Loss   21.0234, KL-Weight  0.849\n",
      "TEST Epoch 09/10, Mean ELBO   90.2058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = run_experiment(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate samples and interpolations\n",
    "# test(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
