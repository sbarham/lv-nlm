- Need to enable switching between multiple corpora with ease
--> o should be super simple interface, transparent to user
--> o should allow easy selection of (1) max vocab, (2) max seq length
--> o should print out stats on resulting dataset after preprocessing

- Problem:
--> x existing dataset code is poorly written, hard to understand, brittle and slow
--> updating code is slow, because care has to be taken not to break anything already present

--> o create code for loading pretrained GLOVE vectors
--> o integrating storing GLOVE vectors with vocab creation
--> o adding easy option for using these pretrained vectors instead of jointly training
- Resolved (4 hours later)
--> code refactored
--> sentences filtered on length
--> pretrained glove embeddings loaded on option

- Problem:
--> x root directory way overcomplicated
--> x project doesn't use best-practice Python standards
    - no pip requirements
    - no license, etc.
- Resolved:
--> x organized code into logical packages (corpus, model, etc.)

- Tested:
--> x all functionality (integration test)
    - mysterious cuda error (59), very hard to track down
    - turned out to be an off-by-one error (indices in w2i ended up being shifted
      over by one vis a vis i2w) leading to the cuda device assert error; the relevant code was:
            special_tokens = ['<unk>', '<pad>', '<sos>', '<eos>']
            for tok in special_tokens:
                i2w[len(i2w)] = tok
                w2i[tok] = len(i2w)
      where the final `len(i2w)` should be `len(w2i)`; and again here:
            if count >= self.min_occ and tok not in special_tokens:
                    i2w[len(i2w)] = tok
                    w2i[tok] = len(w2i)
      with the equivalent correction

--> o model trains and samples as before (but took 3 hours to track down all the relevant errors)

- Problem
--> x need Github repo for all future LV-NLM projects (latent-variable neural language model projects)
--> o created such, including license and README.md

- Problem:
--> x need to enable other corpora to be substituted with ease
--> o created class hierarchy with corpus.base.Corpus as base class for all corpora; this allowed
      much code to be factored out of the other Corpus classes
--> o created subclasses PTB (for Mikolov Penn Treebank), Brown (for NLTK Brown), Gutenberg (for NLTK Gutenberg),
      Bible (from NLTK Gutenberg) Wikitext2 (for Wikitext2) and Wikitext103 (for the larger Wikitext corpus)
      - the NLTK corpora have no train/test/val split built in, so a system had to be engineered to simulate this
        in a repeatable way
      - used random.seed() with a project-wide seed value
      - Corpus object automatically creates train/test/val split and pickles the indices for future use
--> o uploaded datasets to Floydhub using command line tool (floyd-cli) to ensure separation of datasets from
      model code (and enable quicker workspace loading)
      
- Problem:
--> o migrated to Pytorch 1.0 ... you'd think that'd be a good thing? but
--> x as a side-effect of the variable/tensor merge (and how old and bad the code base was),
      there are numerous
          "invalid index of a 0-dim tensor"
      warnings-turned-errors, and the code breaks on FloydHub
--> o the solution amounted essentially to hunting down every occurrence of `.data[0]`
      and converting it to `.item()`
      - no doubt there are still other outdated Torch idioms/usages scattered throughout the code,
        but I haven't the time to hunt them down at the moment
        
- Problem:
--> x mysterious error upon log_softmax() in bowman.SentenceVAE#forward() function
      - I'd give more info, but Python claims precise stacktrace lost "due to multithreading" (yikes)
--> o this was fixed eventually by walking back to PyTorch 0.4

- Problem
--> x upon testing other datasets (in particular the NLTK ones), discovered a number of errors
--> o took a few hours to track them all down
      - caveat: uncertain whether wikitext-103 works or not; the dataset consists of > 100 million
                word tokens, and after waiting 15 minutes just to process the corpus' vocabulary,
                I gave up, to work on more pressing issues

- Problem (TODO)
--> x 'bidirectional' setting does not work (hidden dimension mismatch)
       - not yet resolved