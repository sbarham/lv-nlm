- Need to enable switching between multiple corpora with ease
--> o should be super simple interface, transparent to user
--> o should allow easy selection of (1) max vocab, (2) max seq length
--> o should print out stats on resulting dataset after preprocessing

- Problem:
--> x existing dataset code is poorly written, hard to understand, brittle and slow
--> updating code is slow, because care has to be taken not to break anything already present

--> o create code for loading pretrained GLOVE vectors
--> o integrating storing GLOVE vectors with vocab creation
--> o adding easy option for using these pretrained vectors instead of jointly training
- Resolved (4 hours later)
--> code refactored
--> sentences filtered on length
--> pretrained glove embeddings loaded on option

- Problem:
--> x root directory way overcomplicated
--> x project doesn't use best-practice Python standards
    - no pip requirements
    - no license, etc.
- Resolved:
--> x organized code into logical packages (corpus, model, etc.)

- Tested:
--> x all functionality (integration test)
    - mysterious cuda error (59), very hard to track down
    - turned out to be an off-by-one error (indices in w2i ended up being shifted
      over by one vis a vis i2w) leading to the cuda device assert error; the relevant code was:
            special_tokens = ['<unk>', '<pad>', '<sos>', '<eos>']
            for tok in special_tokens:
                i2w[len(i2w)] = tok
                w2i[tok] = len(i2w)
      where the final `len(i2w)` should be `len(w2i)`; and again here:
            if count >= self.min_occ and tok not in special_tokens:
                    i2w[len(i2w)] = tok
                    w2i[tok] = len(w2i)
      with the equivalent correction

--> o model trains and samples as before (but took 3 hours to track down all the relevant errors)

- Problem
--> x need Github repo for all future LV-NLM projects (latent-variable neural language model projects)
--> o created such, including license and README.md

- Problem:
--> x need to enable other corpora to be substituted with ease
--> o created class hierarchy with corpus.base.Corpus as base class for all corpora; this allowed
      much code to be factored out of the other Corpus classes
--> o created subclasses PTB (for Mikolov Penn Treebank), Brown (for NLTK Brown), Gutenberg (for NLTK Gutenberg),
      Bible (from NLTK Gutenberg) Wikitext2 (for Wikitext2) and Wikitext103 (for the larger Wikitext corpus)
      - the NLTK corpora have no train/test/val split built in, so a system had to be engineered to simulate this
        in a repeatable way
      - used random.seed() with a project-wide seed value
      - Corpus object automatically creates train/test/val split and pickles the indices for future use
--> o uploaded datasets to Floydhub using command line tool (floyd-cli) to ensure separation of datasets from
      model code (and enable quicker workspace loading)